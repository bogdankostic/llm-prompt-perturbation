{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89ac662",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_model = {\n",
    "    \"openai/gpt-oss-120b\": \"GPT-OSS-120b\",\n",
    "    \"gpt-5-mini-2025-08-07\": \"GPT-5-mini\",\n",
    "    \"gemini-2.5-flash\": \"gemini-2.5-flash\",\n",
    "    \"openai/gpt-oss-20b\": \"GPT-OSS-20b\",\n",
    "    \"Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\": \"Qwen3-235B-A22B-Instruct-2507\",\n",
    "    \"gpt-5-nano-2025-08-07\": \"GPT-5-Nano\",\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\": \"Qwen3-30B-A3B-Instruct-2507\",\n",
    "    \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\": \"Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    \"gemini-2.5-flash-lite\": \"gemini-2.5-flash-lite\",\n",
    "    \"gpt-4.1-mini-2025-04-14\": \"GPT-4.1-mini\",\n",
    "    \"google/gemma-3-27b-it\": \"gemma-3-27b-it\",\n",
    "    \"mistralai/Mistral-Large-Instruct-2411\": \"Mistral-Large-Instruct-2411\",\n",
    "    \"google/gemma-3-12b-it\": \"gemma-3-12b-it\",\n",
    "    \"gpt-4.1-nano-2025-04-14\": \"GPT-4.1-Nano\",\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\": \"Qwen3-4B-Instruct-2507\",\n",
    "    \"RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8\": \"Llama-3.3-70B-Instruct\",\n",
    "    \"google/gemma-3-4b-it\": \"gemma-3-4b-it\",\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\": \"Ministral-8B-Instruct-2410\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\": \"Llama-3.1-8B-Instruct\",\n",
    "    \"google/gemma-3-1b-it\": \"gemma-3-1b-it\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": \"Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": \"Llama-3.2-1B-Instruct\",\n",
    "    \"google/gemma-3-270m-it\": \"gemma-3-270m-it\",\n",
    "}\n",
    "\n",
    "model_order = [\n",
    "    \"GPT-5-Nano\",\n",
    "    \"GPT-5-mini\",\n",
    "    \"GPT-4.1-Nano\",\n",
    "    \"GPT-4.1-mini\",\n",
    "    \"GPT-OSS-120b\",\n",
    "    \"GPT-OSS-20b\",\n",
    "    \"Llama-3.3-70B-Instruct\",\n",
    "    \"Llama-3.1-8B-Instruct\",\n",
    "    \"Llama-3.2-3B-Instruct\",\n",
    "    \"Llama-3.2-1B-Instruct\",\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "    \"gemma-3-27b-it\",\n",
    "    \"gemma-3-12b-it\",\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-1b-it\",\n",
    "    \"gemma-3-270m-it\",\n",
    "    \"Mistral-Large-Instruct-2411\",\n",
    "    \"Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    \"Ministral-8B-Instruct-2410\",\n",
    "    \"Qwen3-235B-A22B-Instruct-2507\",\n",
    "    \"Qwen3-30B-A3B-Instruct-2507\",\n",
    "    \"Qwen3-4B-Instruct-2507\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ea34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "def update_amega_metrics(amega_experiment_file, amega_criteria_df):\n",
    "    \"\"\"\n",
    "    Compute per-case scores and mean score for AMEGA predictions and write them back to the JSON file.\n",
    "    \"\"\"\n",
    "    # Read source JSON\n",
    "    with open(amega_experiment_file, \"r\") as f:\n",
    "        amega_data = json.load(f)\n",
    "    majority_votes = amega_data[\"predictions\"][\"majority_vote\"]\n",
    "\n",
    "    # Compute case scores\n",
    "    current_case_id = None\n",
    "    current_case_score = 0.0\n",
    "    case_scores = []\n",
    "\n",
    "    for (_, criterion_row), criterion_met in zip(amega_criteria_df.iterrows(), chain.from_iterable(majority_votes)):\n",
    "        row_case_id = criterion_row[\"case_id\"]\n",
    "        row_score_possible = criterion_row[\"criteria_score_possible\"]\n",
    "\n",
    "        if current_case_id is None:\n",
    "            current_case_id = row_case_id\n",
    "\n",
    "        # New case encountered: close out the previous one\n",
    "        if row_case_id != current_case_id:\n",
    "            case_scores.append(current_case_score)\n",
    "            current_case_id = row_case_id\n",
    "            current_case_score = 0.0\n",
    "\n",
    "        if criterion_met:\n",
    "            current_case_score += row_score_possible\n",
    "\n",
    "    # Append the final case's score\n",
    "    if current_case_id is not None:\n",
    "        case_scores.append(current_case_score)\n",
    "\n",
    "    mean_score = mean(case_scores)\n",
    "\n",
    "    # Write back into JSON\n",
    "    amega_data.setdefault(\"metrics\", {})\n",
    "    amega_data[\"metrics\"][\"case_scores\"] = case_scores\n",
    "    amega_data[\"metrics\"][\"mean_score\"] = mean_score\n",
    "\n",
    "    with open(amega_experiment_file, \"w\") as f:\n",
    "        json.dump(amega_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29c82c",
   "metadata": {},
   "source": [
    "# Lexical Perturbations\n",
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42745dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Identify changed rows\n",
    "mmlu_data_dir = \"../../data/mmlu\"\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"original.json\")) as f:\n",
    "    original_data = json.load(f)\n",
    "original_data_df = pd.DataFrame(original_data[\"data\"])\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if row[\"question\"] != original_data_df.iloc[index][\"question\"] or row[\"choices\"] != original_data_df.iloc[index][\"choices\"]:\n",
    "        changed_rows.append(index)\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "results_dir = \"../../results\"\n",
    "\n",
    "model_names = []\n",
    "original_scores = []\n",
    "lexical_scores = []\n",
    "score_differences = []\n",
    "for model_dir in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model_dir)):\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        original_results_df[\"correct\"] = original_results_df[\"prediction\"] == original_results_df[\"answer\"]\n",
    "        original_score = original_results_df[\"correct\"].mean()\n",
    "        original_scores.append(original_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"lexical\", \"llm_synonym.json\")) as f:\n",
    "            lexical_results = json.load(f)\n",
    "        lexical_results_df = pd.DataFrame(lexical_results[\"predictions\"]).iloc[changed_rows]\n",
    "        lexical_results_df[\"correct\"] = lexical_results_df[\"prediction\"] == lexical_results_df[\"answer\"]\n",
    "        lexical_score = lexical_results_df[\"correct\"].mean()\n",
    "        lexical_scores.append(lexical_score)\n",
    "\n",
    "        score_differences.append(original_score - lexical_score)\n",
    "\n",
    "mmlu_df_lexical = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": original_scores,\n",
    "    \"lexical\": lexical_scores,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "mmlu_df_lexical[\"model\"] = mmlu_df_lexical[\"model\"].map(id_to_model)\n",
    "mmlu_df_lexical = mmlu_df_lexical.set_index(\"model\").reindex(model_order).reset_index()\n",
    "mmlu_df_lexical[\"rank_original\"] = mmlu_df_lexical[\"original\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_lexical[\"rank_lexical\"] = mmlu_df_lexical[\"lexical\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_lexical.to_csv(\"../../data/result_tables/mmlu_lexical.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01a8a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 69.62\\% & 59.44\\% & 10.18pp \\\\\\addlinespace[3pt]\n",
      "GPT-5-mini & 80.07\\% & 70.78\\% & 9.29pp \\\\\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 69.97\\% & 62.16\\% & 7.81pp \\\\\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 80.76\\% & 72.51\\% & 8.25pp \\\\\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 86.20\\% & 76.48\\% & 9.71pp \\\\\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 81.46\\% & 71.87\\% & 9.59pp \\\\\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 80.49\\% & 71.22\\% & 9.27pp \\\\\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 62.86\\% & 54.78\\% & 8.08pp \\\\\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 58.05\\% & 51.03\\% & 7.02pp \\\\\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 25.71\\% & 24.87\\% & 0.85pp \\\\\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 84.97\\% & 76.19\\% & 8.78pp \\\\\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 63.49\\% & 54.91\\% & 8.59pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 76.63\\% & 67.17\\% & 9.46pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 71.19\\% & 62.52\\% & 8.68pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 57.25\\% & 51.56\\% & 5.69pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 38.83\\% & 36.12\\% & 2.71pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 13.38\\% & 13.70\\% & -0.32pp \\\\\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 80.06\\% & 70.71\\% & 9.35pp \\\\\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 76.66\\% & 67.33\\% & 9.33pp \\\\\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 61.70\\% & 54.11\\% & 7.59pp \\\\\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 85.59\\% & 76.15\\% & 9.44pp \\\\\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 77.32\\% & 68.88\\% & 8.44pp \\\\\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 66.05\\% & 56.34\\% & 9.71pp \\\\\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in mmlu_df_lexical.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']*100:.2f}\\\\% & {row['lexical']*100:.2f}\\\\% & {row['score_difference']*100:.2f}pp \\\\\\\\\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089f121",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e67afc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5a3637fa684edb9c96fc781fea9080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cba4bc4558644b19a1fbb5b9e7676be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a599ed1a634857b36505b0b7f4827a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bb3896ca6042c1a2cc1b1bea7525d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a4cd67bab6431ebb88f10241ff658b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b715425f32c84a33a027a1f5a6b1978c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e25bfad5264dd8804d45f8efb942ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e491e65fac7a405d867afbbb5195f7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4798166d94754cb7b4a5d5001bed68eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83cc033752844fb490a1e5fe598e3efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a510fa8edca84fba92d939638086450b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8703aa5daf409584b625e27d9c82bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00435752313b4a998c8a598da01aa4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c6f598ba3c4b9595dbd2d0ab82fc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a76e2ce85046319d36da108316f8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20ff8021437446ea35810c538868dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1298037b5b44112b08fdec205894255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4f207c06be46a38c1131dbfba0822b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e3286df41745ada5113f7f3d213ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae6711918794867a06edccbe4b56b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dd6b5a1cf14dad9631360c1382862b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3685c004a4841f79b7b62c4f90a83b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4bface9bde4557a86dfda3d0e2cd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58a47969322454d9f35b8ebd09033d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99285b254bbc47119744b0437591c679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3390c2c00dfc4ce4b895bff990dfa667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6f7c7083a34ed486d7c63f1ac73f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a318ea243f4501acfc263cb07252b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c2893adba1406ca093cdf1fc389476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec457280c8b431ca50c65b09eae32a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a028ccf60dbf4025825b0e60550b3663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64667d99ee5f4df995d3067072dc9919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6687110eb4a146969c26b3c593cf6573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f66732a2a214e0bb987a56ebee8deb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900a01a52a3a4a088b24156fea3712b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e4e8c5458f4298b0da73e0a5accf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a6ae2232454bd387c3b49f18d8e0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403999285a304cc2bfe6cfc86aaf6aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14172587e03346d9bea2fd1a82955f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeff78a9be3e44929b7d704b5cacc1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87795d5439d44b519a85482b702793e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce60d6578ffe4b1ba65545c4d7ee6f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4d399e4cc8499eadcb465154985716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85d81b054734a3a984767331c3c5b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c096fb5554c441bac4009d04851bfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c82966f56a46559c5bd1f9f89f8f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a19148884e4c54b3141e6285f9c7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "from haystack.components.evaluators import SASEvaluator\n",
    "from haystack.utils import ComponentDevice\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Identify changed rows\n",
    "squad_data = \"../../data/squad\"\n",
    "original_data_df = load_dataset(\"rajpurkar/squad\", split=\"validation\").shuffle(seed=77).select(range(1000)).to_pandas()\n",
    "with open(os.path.join(squad_data, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if (row[\"question\"] != original_data_df.iloc[index][\"question\"]) or (row[\"context\"] != original_data_df.iloc[index][\"context\"]):\n",
    "        changed_rows.append(index)\n",
    "\n",
    "# Initialize evaluators\n",
    "squad_evaluator = evaluate.load(\"squad\")\n",
    "sas_evaluator = SASEvaluator(device=ComponentDevice.from_str(\"mps\"))\n",
    "sas_evaluator.warm_up()\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "model_names = []\n",
    "original_em_scores = []\n",
    "original_f1_scores = []\n",
    "original_sas_scores = []\n",
    "lexical_em_scores = []\n",
    "lexical_f1_scores = []\n",
    "lexical_sas_scores = []\n",
    "difference_em = []\n",
    "difference_f1 = []\n",
    "difference_sas = []\n",
    "for model in tqdm(os.listdir(results_dir), desc=\"Processing models\"):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(original_results_df.iterrows(), desc=\"Processing original rows\", total=len(original_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        original_em_score = statistics.mean(em_scores)\n",
    "        original_f1_score = statistics.mean(f1_scores)\n",
    "        original_sas_score = statistics.mean(sas_scores)\n",
    "        original_em_scores.append(original_em_score)\n",
    "        original_f1_scores.append(original_f1_score)\n",
    "        original_sas_scores.append(original_sas_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"lexical\", \"llm_synonym.json\")) as f:\n",
    "            lexical_results = json.load(f)\n",
    "        lexical_results_df = pd.DataFrame(lexical_results[\"predictions\"]).iloc[changed_rows]\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(lexical_results_df.iterrows(), desc=\"Processing lexical rows\", total=len(lexical_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        lexical_em_score = statistics.mean(em_scores)\n",
    "        lexical_f1_score = statistics.mean(f1_scores)\n",
    "        lexical_sas_score = statistics.mean(sas_scores)\n",
    "        lexical_em_scores.append(lexical_em_score)\n",
    "        lexical_f1_scores.append(lexical_f1_score)\n",
    "        lexical_sas_scores.append(lexical_sas_score)\n",
    "\n",
    "        difference_em.append(original_em_score - lexical_em_score)\n",
    "        difference_f1.append(original_f1_score - lexical_f1_score)\n",
    "        difference_sas.append(original_sas_score - lexical_sas_score)\n",
    "\n",
    "squad_df_lexical = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original_em\": original_em_scores,\n",
    "    \"original_f1\": original_f1_scores,\n",
    "    \"original_sas\": original_sas_scores,\n",
    "    \"lexical_em\": lexical_em_scores,\n",
    "    \"lexical_f1\": lexical_f1_scores,\n",
    "    \"lexical_sas\": lexical_sas_scores,\n",
    "    \"difference_em\": difference_em,\n",
    "    \"difference_f1\": difference_f1,\n",
    "    \"difference_sas\": difference_sas\n",
    "})\n",
    "squad_df_lexical[\"model\"] = squad_df_lexical[\"model\"].map(id_to_model)\n",
    "squad_df_lexical = squad_df_lexical.set_index(\"model\").reindex(model_order).reset_index()\n",
    "squad_df_lexical[\"rank_original_em\"] = squad_df_lexical[\"original_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_original_f1\"] = squad_df_lexical[\"original_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_original_sas\"] = squad_df_lexical[\"original_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_lexical_em\"] = squad_df_lexical[\"lexical_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_lexical_f1\"] = squad_df_lexical[\"lexical_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_lexical_sas\"] = squad_df_lexical[\"lexical_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical.to_csv(\"../../data/result_tables/squad_lexical.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5658a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 66.43\\% & 62.92\\% & 3.50pp & 83.67\\% & 80.11\\% & 3.55pp & 90.98\\% & 88.81\\% & 2.17pp\\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 75.08\\% & 71.06\\% & 4.02pp & 89.44\\% & 85.99\\% & 3.45pp & 93.68\\% & 92.07\\% & 1.61pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 76.73\\% & 71.47\\% & 5.25pp & 89.56\\% & 85.60\\% & 3.96pp & 93.98\\% & 91.91\\% & 2.07pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 77.03\\% & 72.50\\% & 4.53pp & 90.59\\% & 86.85\\% & 3.74pp & 94.36\\% & 92.51\\% & 1.85pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 71.78\\% & 67.35\\% & 4.43pp & 87.18\\% & 83.73\\% & 3.44pp & 93.37\\% & 91.48\\% & 1.89pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 70.75\\% & 65.91\\% & 4.84pp & 87.09\\% & 83.05\\% & 4.04pp & 92.36\\% & 90.54\\% & 1.83pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 82.18\\% & 77.14\\% & 5.05pp & 92.50\\% & 88.96\\% & 3.53pp & 95.86\\% & 94.00\\% & 1.86pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 72.19\\% & 66.94\\% & 5.25pp & 86.51\\% & 83.16\\% & 3.35pp & 91.95\\% & 90.36\\% & 1.60pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 75.28\\% & 71.88\\% & 3.40pp & 87.17\\% & 84.13\\% & 3.05pp & 92.66\\% & 91.15\\% & 1.51pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 57.26\\% & 53.04\\% & 4.22pp & 71.21\\% & 67.11\\% & 4.09pp & 84.17\\% & 81.73\\% & 2.45pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 86.41\\% & 83.52\\% & 2.88pp & 94.21\\% & 91.68\\% & 2.52pp & 96.76\\% & 95.57\\% & 1.19pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 85.58\\% & 80.33\\% & 5.25pp & 93.48\\% & 89.56\\% & 3.92pp & 96.16\\% & 94.39\\% & 1.77pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 76.21\\% & 71.88\\% & 4.33pp & 90.08\\% & 87.20\\% & 2.88pp & 93.90\\% & 92.44\\% & 1.46pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 81.77\\% & 77.55\\% & 4.22pp & 91.43\\% & 88.58\\% & 2.85pp & 95.08\\% & 93.62\\% & 1.46pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 78.89\\% & 74.77\\% & 4.12pp & 89.61\\% & 86.59\\% & 3.01pp & 93.62\\% & 92.18\\% & 1.45pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 64.98\\% & 60.76\\% & 4.22pp & 77.68\\% & 73.26\\% & 4.42pp & 87.28\\% & 85.20\\% & 2.08pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 21.11\\% & 20.19\\% & 0.93pp & 36.16\\% & 33.52\\% & 2.64pp & 61.20\\% & 59.21\\% & 1.99pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 86.92\\% & 83.11\\% & 3.81pp & 93.31\\% & 90.50\\% & 2.80pp & 96.49\\% & 95.48\\% & 1.01pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 84.24\\% & 79.20\\% & 5.05pp & 92.91\\% & 89.48\\% & 3.43pp & 95.80\\% & 93.91\\% & 1.89pp\\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 80.02\\% & 75.80\\% & 4.22pp & 89.09\\% & 85.36\\% & 3.73pp & 93.47\\% & 91.70\\% & 1.77pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 70.75\\% & 68.69\\% & 2.06pp & 85.63\\% & 83.74\\% & 1.89pp & 91.96\\% & 90.79\\% & 1.18pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 76.00\\% & 71.58\\% & 4.43pp & 88.73\\% & 85.73\\% & 3.00pp & 93.70\\% & 91.97\\% & 1.73pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 69.10\\% & 63.34\\% & 5.77pp & 85.01\\% & 80.61\\% & 4.40pp & 91.38\\% & 88.74\\% & 2.64pp\\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in squad_df_lexical.iterrows():\n",
    "    print(f\"{row['model']} & {row['original_em']:.2f}\\\\% & {row['lexical_em']:.2f}\\\\% & {row['difference_em']:.2f}pp & {row['original_f1']:.2f}\\\\% & {row['lexical_f1']:.2f}\\\\% & {row['difference_f1']:.2f}pp & {row['original_sas']*100:.2f}\\\\% & {row['lexical_sas']*100:.2f}\\\\% & {row['difference_sas']*100:.2f}pp\\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9fa84",
   "metadata": {},
   "source": [
    "## AMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e671d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AMEGA score based on weighted criteria\n",
    "amega_criteria_df = pd.read_csv(\"../../AMEGA-benchmark/data/criteria.csv\", sep=\";\", decimal=\",\")\n",
    "amega_criteria_df[\"criteria_score_possible\"] = amega_criteria_df[\"criteria_score_possible\"].astype(float)\n",
    "\n",
    "for item in os.listdir(results_dir):\n",
    "    current_dir = os.path.join(results_dir, item)\n",
    "    if os.path.isdir(current_dir):\n",
    "        original_amega_file = os.path.join(current_dir, \"amega/original_v3.json\")\n",
    "        syntactic_variation_file = os.path.join(current_dir, \"amega/syntactic/syntactic_v3.json\")\n",
    "        lexical_variation_file = os.path.join(current_dir, \"amega/lexical/llm_synonym_v3.json\")\n",
    "        \n",
    "        update_amega_metrics(original_amega_file, amega_criteria_df)\n",
    "        update_amega_metrics(syntactic_variation_file, amega_criteria_df)\n",
    "        update_amega_metrics(lexical_variation_file, amega_criteria_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d28391c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for each model\n",
    "model_names = []\n",
    "amega_scores_original = []\n",
    "amega_scores_lexical = []\n",
    "score_differences = []\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"original_v3.json\")) as f:\n",
    "            original_amega_data = json.load(f)\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"lexical\", \"llm_synonym_v3.json\")) as f:\n",
    "            lexical_amega_data = json.load(f)\n",
    "\n",
    "        model_names.append(original_amega_data[\"model\"])\n",
    "        original_score = original_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        lexical_score = lexical_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        amega_scores_original.append(original_score)\n",
    "        amega_scores_lexical.append(lexical_score)\n",
    "        score_differences.append(original_score - lexical_score)\n",
    "\n",
    "amega_df_lexical = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": amega_scores_original,\n",
    "    \"lexical\": amega_scores_lexical,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "amega_df_lexical[\"model\"] = amega_df_lexical[\"model\"].map(id_to_model)\n",
    "amega_df_lexical = amega_df_lexical.set_index(\"model\").reindex(model_order).reset_index()\n",
    "amega_df_lexical[\"rank_original\"] = amega_df_lexical[\"original\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_lexical[\"rank_lexical\"] = amega_df_lexical[\"lexical\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_lexical.to_csv(\"../../data/result_tables/amega_lexical.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "403d0fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 37.44 & 35.51 & 1.93 \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 39.64 & 37.49 & 2.14 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 34.12 & 33.41 & 0.72 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 35.99 & 35.65 & 0.34 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 39.83 & 39.36 & 0.47 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 37.74 & 36.07 & 1.68 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 32.70 & 32.21 & 0.49 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 29.80 & 28.35 & 1.45 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 26.58 & 25.28 & 1.30 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 22.19 & 19.71 & 2.48 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 38.05 & 37.94 & 0.11 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 36.04 & 34.96 & 1.07 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 35.40 & 34.20 & 1.20 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 35.14 & 34.41 & 0.73 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 32.72 & 31.51 & 1.21 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 26.89 & 26.17 & 0.72 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 15.71 & 11.70 & 4.01 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 34.85 & 32.47 & 2.39 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 36.78 & 35.27 & 1.51 \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 30.79 & 29.30 & 1.50 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 37.27 & 37.02 & 0.25 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 36.65 & 36.58 & 0.07 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 34.22 & 33.20 & 1.03 \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in amega_df_lexical.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']:.2f} & {row['lexical']:.2f} & {row['score_difference']:.2f} \\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0082de",
   "metadata": {},
   "source": [
    "# Syntactic Perturbations\n",
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd905388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify changed rows\n",
    "mmlu_data_dir = \"../../data/mmlu\"\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"original.json\")) as f:\n",
    "    original_data = json.load(f)\n",
    "original_data_df = pd.DataFrame(original_data[\"data\"])\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"syntactic\", \"syntactic_perturbation.json\")) as f:\n",
    "    syntactic_data = json.load(f)\n",
    "syntactic_data_df = pd.DataFrame(syntactic_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in syntactic_data_df.iterrows():\n",
    "    if row[\"question\"] != original_data_df.iloc[index][\"question\"] or row[\"choices\"] != original_data_df.iloc[index][\"choices\"]:\n",
    "        changed_rows.append(index)\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "results_dir = \"../../results\"\n",
    "\n",
    "model_names = []\n",
    "original_scores = []\n",
    "syntactic_scores = []\n",
    "score_differences = []\n",
    "for model_dir in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model_dir)):\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        original_results_df[\"correct\"] = original_results_df[\"prediction\"] == original_results_df[\"answer\"]\n",
    "        original_score = original_results_df[\"correct\"].mean()\n",
    "        original_scores.append(original_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"syntactic\", \"syntactic.json\")) as f:\n",
    "            syntactic_results = json.load(f)\n",
    "        syntactic_results_df = pd.DataFrame(syntactic_results[\"predictions\"]).iloc[changed_rows]\n",
    "        syntactic_results_df[\"correct\"] = syntactic_results_df[\"prediction\"] == syntactic_results_df[\"answer\"]\n",
    "        syntactic_score = syntactic_results_df[\"correct\"].mean()\n",
    "        syntactic_scores.append(syntactic_score)\n",
    "\n",
    "        score_differences.append(original_score - syntactic_score)\n",
    "\n",
    "mmlu_df_syntactic = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": original_scores,\n",
    "    \"syntactic\": syntactic_scores,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "mmlu_df_syntactic[\"model\"] = mmlu_df_syntactic[\"model\"].map(id_to_model)\n",
    "mmlu_df_syntactic = mmlu_df_syntactic.set_index(\"model\").reindex(model_order).reset_index()\n",
    "mmlu_df_syntactic[\"rank_original\"] = mmlu_df_syntactic[\"original\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_syntactic[\"rank_syntactic\"] = mmlu_df_syntactic[\"syntactic\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_syntactic.to_csv(\"../../data/result_tables/mmlu_syntactic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c7fc9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 65.41\\% & 63.20\\% & 2.21pp \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 77.25\\% & 75.41\\% & 1.84pp \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 65.59\\% & 63.61\\% & 1.98pp \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 78.04\\% & 76.42\\% & 1.61pp \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 82.90\\% & 80.52\\% & 2.39pp \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 77.75\\% & 75.43\\% & 2.32pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 79.69\\% & 78.08\\% & 1.61pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 60.68\\% & 58.35\\% & 2.33pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 56.26\\% & 54.28\\% & 1.98pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 25.60\\% & 25.49\\% & 0.11pp \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 82.97\\% & 80.84\\% & 2.13pp \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 61.52\\% & 61.24\\% & 0.28pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 73.41\\% & 71.87\\% & 1.55pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 67.76\\% & 65.39\\% & 2.37pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 53.09\\% & 51.84\\% & 1.25pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 36.09\\% & 35.26\\% & 0.83pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 14.78\\% & 15.31\\% & -0.53pp \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 78.61\\% & 76.57\\% & 2.03pp \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 73.13\\% & 71.08\\% & 2.05pp \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 59.33\\% & 56.93\\% & 2.40pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 82.93\\% & 80.99\\% & 1.94pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 73.62\\% & 72.21\\% & 1.41pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 62.89\\% & 61.24\\% & 1.65pp \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in mmlu_df_syntactic.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']*100:.2f}\\\\% & {row['syntactic']*100:.2f}\\\\% & {row['score_difference']*100:.2f}pp \\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c502d7a",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dadb702a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c181c116e1654deab83348d7d17ce9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d9d3d0097541ba906e34962b577904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59f959371644a1cb13a484ca6bc1639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba409de650d248719d419df8e3b9ba78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa1e15a644b41f29ee318e3e0771588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b10a497a754c46a5e3d48009e397c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5cbe0a131e4c1b87a9ce41502cf56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa35c05332e44da9a745e493f748f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b9ed43a27c4a8c9159d4ec92cd17a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4832467fbeeb43a498bc03199eb03746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e1ccdb2aec4fb5aca9c6ad1c2a4b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6679a268d545f6877d8064fdebb00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb280bc11804fcea6cd968f1c3cc2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e262292e3f644c9e8dc3635b774f506f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a062260a88a418c90e35d409bcc29c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9fe479b4c54dd3b9d9f900921c69f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cb25ac85304584aa0b54c7969d6f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0648a0a7730f4e77bf08c2d952513ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732995127f904013a1975a24933d6dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9125cca0e00b4ddf93e8a6860279d8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd43e83c6274643b2fb5e5daae28d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8aab51f179640328f7462d57bf668f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2633c2d591740c2a4383a29150b2648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c30ca36e5941209f2ff233956a62f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac93c4bbbf94dda98e0d10fa7ed9ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0d7fbfa193431893c38bf1bf1bd886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad526cd4490b4996b3ad372b5d6acc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64dda620196e4e71b3a982dbfeb00b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd065bca63b41e48916267cd110ac29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c679468dc447c3836521c8fcae328a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f1b82ef7254336bc2d1659073d1b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebc76da7baa4c22819b0c26f202b2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5df99458463460c99a4877c8d7975d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddf4b77ffce446d98fcdbf817819f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff94b7fbd1a40e08b3bfd40886196cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40c7e0ff95e489c9461b8ef0a760cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dcae70dcf24e51856119abcf5f0945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d324d2d8fe5b4704babd3d23ce059d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4719d7b0794b49bdafe07a914da940de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f24995c73de41ba95dfa4cbd4f38498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2091300cfa0c4bcd831af1953fea60a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6357447c678a4036af77626c9920a83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f33b25b531438cb9efd47c54f962e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b390037ae63f4d759377d4a8da059359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1007b9955c564cf597b00b73ac6a0abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e9d235ffc14fdbaca24e08dacb6fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2539e97d08ca4895a36ea2665e4764ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify changed rows\n",
    "squad_data = \"../../data/squad\"\n",
    "original_data_df = load_dataset(\"rajpurkar/squad\", split=\"validation\").shuffle(seed=77).select(range(1000)).to_pandas()\n",
    "with open(os.path.join(squad_data, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if (row[\"question\"] != original_data_df.iloc[index][\"question\"]) or (row[\"context\"] != original_data_df.iloc[index][\"context\"]):\n",
    "        changed_rows.append(index)\n",
    "\n",
    "# Initialize evaluators\n",
    "squad_evaluator = evaluate.load(\"squad\")\n",
    "sas_evaluator = SASEvaluator(device=ComponentDevice.from_str(\"mps\"))\n",
    "sas_evaluator.warm_up()\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "model_names = []\n",
    "original_em_scores = []\n",
    "original_f1_scores = []\n",
    "original_sas_scores = []\n",
    "syntactic_em_scores = []\n",
    "syntactic_f1_scores = []\n",
    "syntactic_sas_scores = []\n",
    "difference_em = []\n",
    "difference_f1 = []\n",
    "difference_sas = []\n",
    "for model in tqdm(os.listdir(results_dir), desc=\"Processing models\"):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(original_results_df.iterrows(), desc=\"Processing original rows\", total=len(original_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        original_em_score = statistics.mean(em_scores)\n",
    "        original_f1_score = statistics.mean(f1_scores)\n",
    "        original_sas_score = statistics.mean(sas_scores)\n",
    "        original_em_scores.append(original_em_score)\n",
    "        original_f1_scores.append(original_f1_score)\n",
    "        original_sas_scores.append(original_sas_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"syntactic\", \"syntactic.json\")) as f:\n",
    "            syntactic_results = json.load(f)\n",
    "        syntactic_results_df = pd.DataFrame(syntactic_results[\"predictions\"]).iloc[changed_rows]\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(syntactic_results_df.iterrows(), desc=\"Processing syntactic rows\", total=len(syntactic_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        syntactic_em_score = statistics.mean(em_scores)\n",
    "        syntactic_f1_score = statistics.mean(f1_scores)\n",
    "        syntactic_sas_score = statistics.mean(sas_scores)\n",
    "        syntactic_em_scores.append(syntactic_em_score)\n",
    "        syntactic_f1_scores.append(syntactic_f1_score)\n",
    "        syntactic_sas_scores.append(syntactic_sas_score)\n",
    "\n",
    "        difference_em.append(original_em_score - syntactic_em_score)\n",
    "        difference_f1.append(original_f1_score - syntactic_f1_score)\n",
    "        difference_sas.append(original_sas_score - syntactic_sas_score)\n",
    "\n",
    "squad_df_syntactic = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original_em\": original_em_scores,\n",
    "    \"original_f1\": original_f1_scores,\n",
    "    \"original_sas\": original_sas_scores,\n",
    "    \"syntactic_em\": syntactic_em_scores,\n",
    "    \"syntactic_f1\": syntactic_f1_scores,\n",
    "    \"syntactic_sas\": syntactic_sas_scores,\n",
    "    \"difference_em\": difference_em,\n",
    "    \"difference_f1\": difference_f1,\n",
    "    \"difference_sas\": difference_sas\n",
    "})\n",
    "squad_df_syntactic[\"model\"] = squad_df_syntactic[\"model\"].map(id_to_model)\n",
    "squad_df_syntactic = squad_df_syntactic.set_index(\"model\").reindex(model_order).reset_index()\n",
    "squad_df_syntactic[\"rank_original_em\"] = squad_df_syntactic[\"original_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_original_f1\"] = squad_df_syntactic[\"original_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_original_sas\"] = squad_df_syntactic[\"original_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_syntactic_em\"] = squad_df_syntactic[\"syntactic_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_syntactic_f1\"] = squad_df_syntactic[\"syntactic_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_syntactic_sas\"] = squad_df_syntactic[\"syntactic_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic.to_csv(\"../../data/result_tables/squad_syntactic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a93633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 66.43\\% & 63.23\\% & 3.19pp & 83.67\\% & 81.06\\% & 2.61pp & 90.98\\% & 89.27\\% & 1.71pp\\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 75.08\\% & 72.09\\% & 2.99pp & 89.44\\% & 87.00\\% & 2.44pp & 93.68\\% & 92.30\\% & 1.38pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 76.73\\% & 73.22\\% & 3.50pp & 89.56\\% & 86.53\\% & 3.03pp & 93.98\\% & 92.51\\% & 1.47pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 77.03\\% & 75.08\\% & 1.96pp & 90.59\\% & 88.32\\% & 2.28pp & 94.36\\% & 93.11\\% & 1.25pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 71.78\\% & 70.03\\% & 1.75pp & 87.18\\% & 85.20\\% & 1.98pp & 93.37\\% & 92.44\\% & 0.93pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 70.75\\% & 68.90\\% & 1.85pp & 87.09\\% & 85.11\\% & 1.98pp & 92.36\\% & 92.05\\% & 0.31pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 82.18\\% & 79.20\\% & 2.99pp & 92.50\\% & 90.23\\% & 2.26pp & 95.86\\% & 94.43\\% & 1.42pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 72.19\\% & 69.62\\% & 2.57pp & 86.51\\% & 84.45\\% & 2.06pp & 91.95\\% & 90.70\\% & 1.25pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 75.28\\% & 73.53\\% & 1.75pp & 87.17\\% & 84.54\\% & 2.63pp & 92.66\\% & 90.98\\% & 1.69pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 57.26\\% & 54.69\\% & 2.57pp & 71.21\\% & 67.30\\% & 3.91pp & 84.17\\% & 81.60\\% & 2.58pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 86.41\\% & 83.52\\% & 2.88pp & 94.21\\% & 91.94\\% & 2.27pp & 96.76\\% & 95.57\\% & 1.18pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 85.58\\% & 82.49\\% & 3.09pp & 93.48\\% & 90.67\\% & 2.81pp & 96.16\\% & 94.43\\% & 1.73pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 76.21\\% & 73.22\\% & 2.99pp & 90.08\\% & 87.25\\% & 2.84pp & 93.90\\% & 92.16\\% & 1.74pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 81.77\\% & 77.55\\% & 4.22pp & 91.43\\% & 88.40\\% & 3.03pp & 95.08\\% & 93.25\\% & 1.83pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 78.89\\% & 75.28\\% & 3.60pp & 89.61\\% & 86.34\\% & 3.27pp & 93.62\\% & 91.51\\% & 2.11pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 64.98\\% & 59.63\\% & 5.36pp & 77.68\\% & 72.36\\% & 5.31pp & 87.28\\% & 84.16\\% & 3.12pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 21.11\\% & 17.30\\% & 3.81pp & 36.16\\% & 30.40\\% & 5.76pp & 61.20\\% & 56.75\\% & 4.46pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 86.92\\% & 85.17\\% & 1.75pp & 93.31\\% & 91.63\\% & 1.68pp & 96.49\\% & 95.27\\% & 1.22pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 84.24\\% & 82.39\\% & 1.85pp & 92.91\\% & 91.03\\% & 1.88pp & 95.80\\% & 94.62\\% & 1.18pp\\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 80.02\\% & 79.40\\% & 0.62pp & 89.09\\% & 87.23\\% & 1.86pp & 93.47\\% & 92.34\\% & 1.13pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 70.75\\% & 69.21\\% & 1.54pp & 85.63\\% & 83.80\\% & 1.82pp & 91.96\\% & 90.53\\% & 1.43pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 76.00\\% & 73.74\\% & 2.27pp & 88.73\\% & 86.47\\% & 2.25pp & 93.70\\% & 92.37\\% & 1.33pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 69.10\\% & 67.46\\% & 1.65pp & 85.01\\% & 82.29\\% & 2.71pp & 91.38\\% & 89.08\\% & 2.30pp\\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in squad_df_syntactic.iterrows():\n",
    "    print(f\"{row['model']} & {row['original_em']:.2f}\\\\% & {row['syntactic_em']:.2f}\\\\% & {row['difference_em']:.2f}pp & {row['original_f1']:.2f}\\\\% & {row['syntactic_f1']:.2f}\\\\% & {row['difference_f1']:.2f}pp & {row['original_sas']*100:.2f}\\\\% & {row['syntactic_sas']*100:.2f}\\\\% & {row['difference_sas']*100:.2f}pp\\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0bb7e",
   "metadata": {},
   "source": [
    "## AMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea7d17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for each model\n",
    "model_names = []\n",
    "amega_scores_original = []\n",
    "amega_scores_syntactic = []\n",
    "score_differences = []\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"original_v3.json\")) as f:\n",
    "            original_amega_data = json.load(f)\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"syntactic\", \"syntactic_v3.json\")) as f:\n",
    "            syntactic_amega_data = json.load(f)\n",
    "\n",
    "        model_names.append(original_amega_data[\"model\"])\n",
    "        original_score = original_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        syntactic_score = syntactic_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        amega_scores_original.append(original_score)\n",
    "        amega_scores_syntactic.append(syntactic_score)\n",
    "        score_differences.append(original_score - syntactic_score)\n",
    "\n",
    "amega_df_syntactic = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": amega_scores_original,\n",
    "    \"syntactic\": amega_scores_syntactic,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "amega_df_syntactic[\"model\"] = amega_df_syntactic[\"model\"].map(id_to_model)\n",
    "amega_df_syntactic = amega_df_syntactic.set_index(\"model\").reindex(model_order).reset_index()\n",
    "amega_df_syntactic[\"rank_original\"] = amega_df_syntactic[\"original\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_syntactic[\"rank_syntactic\"] = amega_df_syntactic[\"syntactic\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_syntactic.to_csv(\"../../data/result_tables/amega_syntactic.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "431f1a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 37.44 & 36.83 & 0.61 \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 39.64 & 38.08 & 1.55 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 34.12 & 33.54 & 0.59 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 35.99 & 35.07 & 0.93 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 39.83 & 39.17 & 0.66 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 37.74 & 37.72 & 0.03 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 32.70 & 32.31 & 0.39 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 29.80 & 29.80 & -0.00 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 26.58 & 27.35 & -0.77 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 22.19 & 21.83 & 0.36 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 38.05 & 36.92 & 1.13 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 36.04 & 34.49 & 1.55 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 35.40 & 35.46 & -0.06 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 35.14 & 35.84 & -0.70 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 32.72 & 32.15 & 0.58 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 26.89 & 26.02 & 0.87 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 15.71 & 14.44 & 1.27 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 34.85 & 33.76 & 1.10 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 36.78 & 35.04 & 1.74 \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 30.79 & 29.87 & 0.92 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 37.27 & 37.21 & 0.05 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 36.65 & 36.88 & -0.23 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 34.22 & 34.35 & -0.13 \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in amega_df_syntactic.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']:.2f} & {row['syntactic']:.2f} & {row['score_difference']:.2f} \\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216deac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
