{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89ac662",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_model = {\n",
    "    \"openai/gpt-oss-120b\": \"GPT-OSS-120b\",\n",
    "    \"gpt-5-mini-2025-08-07\": \"GPT-5-mini\",\n",
    "    \"gemini-2.5-flash\": \"gemini-2.5-flash\",\n",
    "    \"openai/gpt-oss-20b\": \"GPT-OSS-20b\",\n",
    "    \"Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\": \"Qwen3-235B-A22B-Instruct-2507\",\n",
    "    \"gpt-5-nano-2025-08-07\": \"GPT-5-Nano\",\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\": \"Qwen3-30B-A3B-Instruct-2507\",\n",
    "    \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\": \"Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    \"gemini-2.5-flash-lite\": \"gemini-2.5-flash-lite\",\n",
    "    \"gpt-4.1-mini-2025-04-14\": \"GPT-4.1-mini\",\n",
    "    \"google/gemma-3-27b-it\": \"gemma-3-27b-it\",\n",
    "    \"mistralai/Mistral-Large-Instruct-2411\": \"Mistral-Large-Instruct-2411\",\n",
    "    \"google/gemma-3-12b-it\": \"gemma-3-12b-it\",\n",
    "    \"gpt-4.1-nano-2025-04-14\": \"GPT-4.1-Nano\",\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\": \"Qwen3-4B-Instruct-2507\",\n",
    "    \"RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8\": \"Llama-3.3-70B-Instruct\",\n",
    "    \"google/gemma-3-4b-it\": \"gemma-3-4b-it\",\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\": \"Ministral-8B-Instruct-2410\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\": \"Llama-3.1-8B-Instruct\",\n",
    "    \"google/gemma-3-1b-it\": \"gemma-3-1b-it\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": \"Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": \"Llama-3.2-1B-Instruct\",\n",
    "    \"google/gemma-3-270m-it\": \"gemma-3-270m-it\",\n",
    "}\n",
    "\n",
    "model_order = [\n",
    "    \"GPT-5-Nano\",\n",
    "    \"GPT-5-mini\",\n",
    "    \"GPT-4.1-Nano\",\n",
    "    \"GPT-4.1-mini\",\n",
    "    \"GPT-OSS-120b\",\n",
    "    \"GPT-OSS-20b\",\n",
    "    \"Llama-3.3-70B-Instruct\",\n",
    "    \"Llama-3.1-8B-Instruct\",\n",
    "    \"Llama-3.2-3B-Instruct\",\n",
    "    \"Llama-3.2-1B-Instruct\",\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "    \"gemma-3-27b-it\",\n",
    "    \"gemma-3-12b-it\",\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-1b-it\",\n",
    "    \"gemma-3-270m-it\",\n",
    "    \"Mistral-Large-Instruct-2411\",\n",
    "    \"Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    \"Ministral-8B-Instruct-2410\",\n",
    "    \"Qwen3-235B-A22B-Instruct-2507\",\n",
    "    \"Qwen3-30B-A3B-Instruct-2507\",\n",
    "    \"Qwen3-4B-Instruct-2507\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ea34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "def update_amega_metrics(amega_experiment_file, amega_criteria_df):\n",
    "    \"\"\"\n",
    "    Compute per-case scores and mean score for AMEGA predictions and write them back to the JSON file.\n",
    "    \"\"\"\n",
    "    # Read source JSON\n",
    "    with open(amega_experiment_file, \"r\") as f:\n",
    "        amega_data = json.load(f)\n",
    "    majority_votes = amega_data[\"predictions\"][\"majority_vote\"]\n",
    "\n",
    "    # Compute case scores\n",
    "    current_case_id = None\n",
    "    current_case_score = 0.0\n",
    "    case_scores = []\n",
    "\n",
    "    for (_, criterion_row), criterion_met in zip(amega_criteria_df.iterrows(), chain.from_iterable(majority_votes)):\n",
    "        row_case_id = criterion_row[\"case_id\"]\n",
    "        row_score_possible = criterion_row[\"criteria_score_possible\"]\n",
    "\n",
    "        if current_case_id is None:\n",
    "            current_case_id = row_case_id\n",
    "\n",
    "        # New case encountered: close out the previous one\n",
    "        if row_case_id != current_case_id:\n",
    "            case_scores.append(current_case_score)\n",
    "            current_case_id = row_case_id\n",
    "            current_case_score = 0.0\n",
    "\n",
    "        if criterion_met:\n",
    "            current_case_score += row_score_possible\n",
    "\n",
    "    # Append the final case's score\n",
    "    if current_case_id is not None:\n",
    "        case_scores.append(current_case_score)\n",
    "\n",
    "    mean_score = mean(case_scores)\n",
    "\n",
    "    # Write back into JSON\n",
    "    amega_data.setdefault(\"metrics\", {})\n",
    "    amega_data[\"metrics\"][\"case_scores\"] = case_scores\n",
    "    amega_data[\"metrics\"][\"mean_score\"] = mean_score\n",
    "\n",
    "    with open(amega_experiment_file, \"w\") as f:\n",
    "        json.dump(amega_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29c82c",
   "metadata": {},
   "source": [
    "# Lexical Perturbations\n",
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42745dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Identify changed rows\n",
    "mmlu_data_dir = \"../../data/mmlu\"\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"original.json\")) as f:\n",
    "    original_data = json.load(f)\n",
    "original_data_df = pd.DataFrame(original_data[\"data\"])\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if row[\"question\"] != original_data_df.iloc[index][\"question\"] or row[\"choices\"] != original_data_df.iloc[index][\"choices\"]:\n",
    "        changed_rows.append(index)\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "results_dir = \"../../results\"\n",
    "\n",
    "model_names = []\n",
    "original_scores = []\n",
    "lexical_scores = []\n",
    "score_differences = []\n",
    "for model_dir in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model_dir)):\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        original_results_df[\"correct\"] = original_results_df[\"prediction\"] == original_results_df[\"answer\"]\n",
    "        original_score = original_results_df[\"correct\"].mean()\n",
    "        original_scores.append(original_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"lexical\", \"llm_synonym.json\")) as f:\n",
    "            lexical_results = json.load(f)\n",
    "        lexical_results_df = pd.DataFrame(lexical_results[\"predictions\"]).iloc[changed_rows]\n",
    "        lexical_results_df[\"correct\"] = lexical_results_df[\"prediction\"] == lexical_results_df[\"answer\"]\n",
    "        lexical_score = lexical_results_df[\"correct\"].mean()\n",
    "        lexical_scores.append(lexical_score)\n",
    "\n",
    "        score_differences.append(original_score - lexical_score)\n",
    "\n",
    "mmlu_df_lexical = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": original_scores,\n",
    "    \"lexical\": lexical_scores,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "mmlu_df_lexical[\"model\"] = mmlu_df_lexical[\"model\"].map(id_to_model)\n",
    "mmlu_df_lexical = mmlu_df_lexical.set_index(\"model\").reindex(model_order).reset_index()\n",
    "mmlu_df_lexical[\"rank_original\"] = mmlu_df_lexical[\"original\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_lexical[\"rank_lexical\"] = mmlu_df_lexical[\"lexical\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_lexical.to_csv(\"../../data/result_tables/mmlu_lexical.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e01a8a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 69.62\\% & 59.44\\% & 10.18pp \\\\\\addlinespace[3pt]\n",
      "GPT-5-mini & 80.07\\% & 70.78\\% & 9.29pp \\\\\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 69.97\\% & 62.16\\% & 7.81pp \\\\\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 80.76\\% & 72.51\\% & 8.25pp \\\\\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 86.20\\% & 76.48\\% & 9.71pp \\\\\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 81.46\\% & 71.87\\% & 9.59pp \\\\\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 80.49\\% & 71.22\\% & 9.27pp \\\\\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 62.86\\% & 54.78\\% & 8.08pp \\\\\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 58.05\\% & 51.03\\% & 7.02pp \\\\\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 25.71\\% & 24.87\\% & 0.85pp \\\\\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 84.97\\% & 76.19\\% & 8.78pp \\\\\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 63.49\\% & 54.91\\% & 8.59pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 76.63\\% & 67.17\\% & 9.46pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 71.19\\% & 62.52\\% & 8.68pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 57.25\\% & 51.56\\% & 5.69pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 38.83\\% & 36.12\\% & 2.71pp \\\\\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 13.38\\% & 13.70\\% & -0.32pp \\\\\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 80.06\\% & 70.71\\% & 9.35pp \\\\\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 76.66\\% & 67.33\\% & 9.33pp \\\\\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 61.70\\% & 54.11\\% & 7.59pp \\\\\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 85.59\\% & 76.15\\% & 9.44pp \\\\\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 77.32\\% & 68.88\\% & 8.44pp \\\\\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 66.05\\% & 56.34\\% & 9.71pp \\\\\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in mmlu_df_lexical.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']*100:.2f}\\\\% & {row['lexical']*100:.2f}\\\\% & {row['score_difference']*100:.2f}pp \\\\\\\\\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089f121",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e67afc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf32859c499d4ec59ab5be82d90d9e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8782ffeb805b47a29b6382dcca782bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8203c44e6d64da5b516359659ce062b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1262ac35ed4fcd8d045111a685e542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1eda761172245debf07fb4e38c70851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a6999e5e1944fa97521bf1f3b79cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f605619e3a43359904ae06d1c1959d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd871fa68d8401599b7be58f60140f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a0f41db0c34722b02df62b126f0f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130adff4901a4c3e8acb0ad89879e124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4497ddbf53f948f19f11fab2f2e7d8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde33c52393545758d1c0d008f19ecbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0538ccad3b0f4faba9d6cbaedb523129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a0d19b76e6456a8b7d4ad33f96edc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5579713d117643848d77f20d558837fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435732f687cc447094e415d3e6611cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbe103af4ed479c9a94e1abeb6cd821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c2bf85923a44b6a4959515dd9bf756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaef16cd192c4d5186ec2b7f9163f34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5b76d34b1f40af8c103add05c99ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdd6a5d3de84702bc45d1b58cf23015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c952b647bda749c09cff3c51be885ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2298e22ba79f4144ae5b35451260572c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5eac8dad63f44fab2a67994624b1deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bffe568663a4e65b33d3c395bf00744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cf0d0e38514b469c83b1d76dc8c453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc7a2509557474bb6c82504736c6bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f5861fb3034adbb21adf298d188276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791db632991b4674981c73b2ab209424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d228cad72ec4344a37d65875c34ff26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe238a243ea4d3eb160fc8f563bda09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e6d4ba1fc041e1a2d0c66f5a2980c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793ae860d2bf44139b5ee1fc96f396c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ce4263dc6c494d99849b51e4e40f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9fb0cc079044de93e6dcc08d049ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bba7ccfa0e4f67a297225de6197f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860978f6ac114a8c8d7074e0d7bf0d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea8b204a10e4ca381d8088b81a0d9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63882dbcc07149c5bd62bca669c56f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16800fd2a4b44cb1a7d496f23ff07ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e842a06037243f6bf18933595f7cd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0aaa03219344e5817d5483cdea62e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92e62468f4647168e9572933cdb49b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4995c7df594f9b9cd195e44c03ea1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a19ba4628e491eb00f83de02463be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906c7fffdb6043438021070c6d6022fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb50ba1a2f20439699241564e7234aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "from haystack.components.evaluators import SASEvaluator\n",
    "from haystack.utils import ComponentDevice\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Identify changed rows\n",
    "squad_data = \"../../data/squad\"\n",
    "original_data_df = load_dataset(\"rajpurkar/squad\", split=\"validation\").shuffle(seed=77).select(range(1000)).to_pandas()\n",
    "with open(os.path.join(squad_data, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if (row[\"question\"] != original_data_df.iloc[index][\"question\"]) or (row[\"context\"] != original_data_df.iloc[index][\"context\"]):\n",
    "        changed_rows.append(index)\n",
    "\n",
    "# Initialize evaluators\n",
    "squad_evaluator = evaluate.load(\"squad\")\n",
    "sas_evaluator = SASEvaluator(device=ComponentDevice.from_str(\"mps\"))\n",
    "sas_evaluator.warm_up()\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "model_names = []\n",
    "original_em_scores = []\n",
    "original_f1_scores = []\n",
    "original_sas_scores = []\n",
    "lexical_em_scores = []\n",
    "lexical_f1_scores = []\n",
    "lexical_sas_scores = []\n",
    "difference_em = []\n",
    "difference_f1 = []\n",
    "difference_sas = []\n",
    "for model in tqdm(os.listdir(results_dir), desc=\"Processing models\"):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(original_results_df.iterrows(), desc=\"Processing original rows\", total=len(original_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        original_em_score = statistics.mean(em_scores)\n",
    "        original_f1_score = statistics.mean(f1_scores)\n",
    "        original_sas_score = statistics.mean(sas_scores)\n",
    "        original_em_scores.append(original_em_score)\n",
    "        original_f1_scores.append(original_f1_score)\n",
    "        original_sas_scores.append(original_sas_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"lexical\", \"llm_synonym.json\")) as f:\n",
    "            lexical_results = json.load(f)\n",
    "        lexical_results_df = pd.DataFrame(lexical_results[\"predictions\"]).iloc[changed_rows]\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(lexical_results_df.iterrows(), desc=\"Processing lexical rows\", total=len(lexical_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        lexical_em_score = statistics.mean(em_scores)\n",
    "        lexical_f1_score = statistics.mean(f1_scores)\n",
    "        lexical_sas_score = statistics.mean(sas_scores)\n",
    "        lexical_em_scores.append(lexical_em_score)\n",
    "        lexical_f1_scores.append(lexical_f1_score)\n",
    "        lexical_sas_scores.append(lexical_sas_score)\n",
    "\n",
    "        difference_em.append(original_em_score - lexical_em_score)\n",
    "        difference_f1.append(original_f1_score - lexical_f1_score)\n",
    "        difference_sas.append(original_sas_score - lexical_sas_score)\n",
    "\n",
    "squad_df_lexical = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original_em\": original_em_scores,\n",
    "    \"original_f1\": original_f1_scores,\n",
    "    \"original_sas\": original_sas_scores,\n",
    "    \"lexical_em\": lexical_em_scores,\n",
    "    \"lexical_f1\": lexical_f1_scores,\n",
    "    \"lexical_sas\": lexical_sas_scores,\n",
    "    \"difference_em\": difference_em,\n",
    "    \"difference_f1\": difference_f1,\n",
    "    \"difference_sas\": difference_sas\n",
    "})\n",
    "squad_df_lexical[\"model\"] = squad_df_lexical[\"model\"].map(id_to_model)\n",
    "squad_df_lexical = squad_df_lexical.set_index(\"model\").reindex(model_order).reset_index()\n",
    "squad_df_lexical[\"rank_original_em\"] = squad_df_lexical[\"original_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_original_f1\"] = squad_df_lexical[\"original_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_original_sas\"] = squad_df_lexical[\"original_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_lexical_em\"] = squad_df_lexical[\"lexical_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_lexical_f1\"] = squad_df_lexical[\"lexical_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_lexical_sas\"] = squad_df_lexical[\"lexical_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical.to_csv(\"../../data/result_tables/squad_lexical.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5658a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 66.43\\% & 62.92\\% & 3.50pp & 83.67\\% & 80.11\\% & 3.55pp & 90.98\\% & 88.81\\% & 2.17pp\\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 75.08\\% & 71.06\\% & 4.02pp & 89.44\\% & 85.99\\% & 3.45pp & 93.68\\% & 92.07\\% & 1.61pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 76.73\\% & 71.47\\% & 5.25pp & 89.56\\% & 85.60\\% & 3.96pp & 93.98\\% & 91.91\\% & 2.07pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 77.03\\% & 72.50\\% & 4.53pp & 90.59\\% & 86.85\\% & 3.74pp & 94.36\\% & 92.51\\% & 1.85pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 71.78\\% & 67.35\\% & 4.43pp & 87.18\\% & 83.73\\% & 3.44pp & 93.37\\% & 91.48\\% & 1.89pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 70.75\\% & 65.91\\% & 4.84pp & 87.09\\% & 83.05\\% & 4.04pp & 92.36\\% & 90.54\\% & 1.83pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 82.18\\% & 77.14\\% & 5.05pp & 92.50\\% & 88.96\\% & 3.53pp & 95.86\\% & 94.00\\% & 1.86pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 72.19\\% & 66.94\\% & 5.25pp & 86.51\\% & 83.16\\% & 3.35pp & 91.95\\% & 90.36\\% & 1.60pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 75.28\\% & 71.88\\% & 3.40pp & 87.17\\% & 84.13\\% & 3.05pp & 92.66\\% & 91.15\\% & 1.51pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 57.26\\% & 53.04\\% & 4.22pp & 71.21\\% & 67.11\\% & 4.09pp & 84.17\\% & 81.73\\% & 2.45pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 86.41\\% & 83.52\\% & 2.88pp & 94.21\\% & 91.68\\% & 2.52pp & 96.76\\% & 95.57\\% & 1.19pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 85.58\\% & 80.33\\% & 5.25pp & 93.48\\% & 89.56\\% & 3.92pp & 96.16\\% & 94.39\\% & 1.77pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 76.21\\% & 71.88\\% & 4.33pp & 90.08\\% & 87.20\\% & 2.88pp & 93.90\\% & 92.44\\% & 1.46pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 81.77\\% & 77.55\\% & 4.22pp & 91.43\\% & 88.58\\% & 2.85pp & 95.08\\% & 93.62\\% & 1.46pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 78.89\\% & 74.77\\% & 4.12pp & 89.61\\% & 86.59\\% & 3.01pp & 93.62\\% & 92.18\\% & 1.45pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 64.98\\% & 60.76\\% & 4.22pp & 77.68\\% & 73.26\\% & 4.42pp & 87.28\\% & 85.20\\% & 2.08pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 21.11\\% & 20.19\\% & 0.93pp & 36.16\\% & 33.52\\% & 2.64pp & 61.20\\% & 59.21\\% & 1.99pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 86.92\\% & 83.11\\% & 3.81pp & 93.31\\% & 90.50\\% & 2.80pp & 96.49\\% & 95.48\\% & 1.01pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 84.24\\% & 79.20\\% & 5.05pp & 92.91\\% & 89.48\\% & 3.43pp & 95.80\\% & 93.91\\% & 1.89pp\\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 80.02\\% & 75.80\\% & 4.22pp & 89.09\\% & 85.36\\% & 3.73pp & 93.47\\% & 91.70\\% & 1.77pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 70.75\\% & 68.69\\% & 2.06pp & 85.63\\% & 83.74\\% & 1.89pp & 91.96\\% & 90.79\\% & 1.18pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 76.00\\% & 71.58\\% & 4.43pp & 88.73\\% & 85.73\\% & 3.00pp & 93.70\\% & 91.97\\% & 1.73pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 69.10\\% & 63.34\\% & 5.77pp & 85.01\\% & 80.61\\% & 4.40pp & 91.38\\% & 88.74\\% & 2.64pp\\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in squad_df_lexical.iterrows():\n",
    "    print(f\"{row['model']} & {row['original_em']:.2f}\\\\% & {row['lexical_em']:.2f}\\\\% & {row['difference_em']:.2f}pp & {row['original_f1']:.2f}\\\\% & {row['lexical_f1']:.2f}\\\\% & {row['difference_f1']:.2f}pp & {row['original_sas']*100:.2f}\\\\% & {row['lexical_sas']*100:.2f}\\\\% & {row['difference_sas']*100:.2f}pp\\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9fa84",
   "metadata": {},
   "source": [
    "## AMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e671d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AMEGA score based on weighted criteria\n",
    "amega_criteria_df = pd.read_csv(\"../../AMEGA-benchmark/data/criteria.csv\", sep=\";\", decimal=\",\")\n",
    "amega_criteria_df[\"criteria_score_possible\"] = amega_criteria_df[\"criteria_score_possible\"].astype(float)\n",
    "\n",
    "for item in os.listdir(results_dir):\n",
    "    current_dir = os.path.join(results_dir, item)\n",
    "    if os.path.isdir(current_dir):\n",
    "        original_amega_file = os.path.join(current_dir, \"amega/original_v3.json\")\n",
    "        syntactic_variation_file = os.path.join(current_dir, \"amega/syntactic/syntactic_v3.json\")\n",
    "        lexical_variation_file = os.path.join(current_dir, \"amega/lexical/llm_synonym_v3.json\")\n",
    "        \n",
    "        update_amega_metrics(original_amega_file, amega_criteria_df)\n",
    "        update_amega_metrics(syntactic_variation_file, amega_criteria_df)\n",
    "        update_amega_metrics(lexical_variation_file, amega_criteria_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d28391c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for each model\n",
    "model_names = []\n",
    "amega_scores_original = []\n",
    "amega_scores_lexical = []\n",
    "score_differences = []\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"original_v3.json\")) as f:\n",
    "            original_amega_data = json.load(f)\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"lexical\", \"llm_synonym_v3.json\")) as f:\n",
    "            lexical_amega_data = json.load(f)\n",
    "\n",
    "        model_names.append(original_amega_data[\"model\"])\n",
    "        original_score = original_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        lexical_score = lexical_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        amega_scores_original.append(original_score)\n",
    "        amega_scores_lexical.append(lexical_score)\n",
    "        score_differences.append(original_score - lexical_score)\n",
    "\n",
    "amega_df_lexical = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": amega_scores_original,\n",
    "    \"lexical\": amega_scores_lexical,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "amega_df_lexical[\"model\"] = amega_df_lexical[\"model\"].map(id_to_model)\n",
    "amega_df_lexical = amega_df_lexical.set_index(\"model\").reindex(model_order).reset_index()\n",
    "amega_df_lexical[\"rank_original\"] = amega_df_lexical[\"original\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_lexical[\"rank_lexical\"] = amega_df_lexical[\"lexical\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_lexical.to_csv(\"../../data/result_tables/amega_lexical.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "403d0fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 37.44 & 35.51 & 1.93 \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 39.64 & 37.49 & 2.14 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 34.12 & 33.41 & 0.72 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 35.99 & 35.65 & 0.34 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 39.83 & 39.36 & 0.47 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 37.74 & 36.07 & 1.68 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 32.70 & 32.21 & 0.49 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 29.80 & 28.35 & 1.45 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 26.58 & 25.28 & 1.30 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 22.19 & 19.71 & 2.48 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 38.05 & 37.94 & 0.11 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 36.04 & 34.96 & 1.07 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 35.40 & 34.20 & 1.20 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 35.14 & 34.41 & 0.73 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 32.72 & 31.51 & 1.21 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 26.89 & 26.17 & 0.72 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 15.71 & 11.70 & 4.01 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 34.85 & 32.47 & 2.39 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 36.78 & 35.27 & 1.51 \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 30.79 & 29.30 & 1.50 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 37.27 & 37.02 & 0.25 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 36.65 & 36.58 & 0.07 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 34.22 & 33.20 & 1.03 \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in amega_df_lexical.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']:.2f} & {row['lexical']:.2f} & {row['score_difference']:.2f} \\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0082de",
   "metadata": {},
   "source": [
    "# Syntactic Perturbations\n",
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd905388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify changed rows\n",
    "mmlu_data_dir = \"../../data/mmlu\"\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"original.json\")) as f:\n",
    "    original_data = json.load(f)\n",
    "original_data_df = pd.DataFrame(original_data[\"data\"])\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"syntactic\", \"syntactic_perturbation.json\")) as f:\n",
    "    syntactic_data = json.load(f)\n",
    "syntactic_data_df = pd.DataFrame(syntactic_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in syntactic_data_df.iterrows():\n",
    "    if row[\"question\"] != original_data_df.iloc[index][\"question\"] or row[\"choices\"] != original_data_df.iloc[index][\"choices\"]:\n",
    "        changed_rows.append(index)\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "results_dir = \"../../results\"\n",
    "\n",
    "model_names = []\n",
    "original_scores = []\n",
    "syntactic_scores = []\n",
    "score_differences = []\n",
    "for model_dir in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model_dir)):\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        original_results_df[\"correct\"] = original_results_df[\"prediction\"] == original_results_df[\"answer\"]\n",
    "        original_score = original_results_df[\"correct\"].mean()\n",
    "        original_scores.append(original_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"syntactic\", \"syntactic.json\")) as f:\n",
    "            syntactic_results = json.load(f)\n",
    "        syntactic_results_df = pd.DataFrame(syntactic_results[\"predictions\"]).iloc[changed_rows]\n",
    "        syntactic_results_df[\"correct\"] = syntactic_results_df[\"prediction\"] == syntactic_results_df[\"answer\"]\n",
    "        syntactic_score = syntactic_results_df[\"correct\"].mean()\n",
    "        syntactic_scores.append(syntactic_score)\n",
    "\n",
    "        score_differences.append(original_score - syntactic_score)\n",
    "\n",
    "mmlu_df_syntactic = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": original_scores,\n",
    "    \"syntactic\": syntactic_scores,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "mmlu_df_syntactic[\"model\"] = mmlu_df_syntactic[\"model\"].map(id_to_model)\n",
    "mmlu_df_syntactic = mmlu_df_syntactic.set_index(\"model\").reindex(model_order).reset_index()\n",
    "mmlu_df_syntactic[\"rank_original\"] = mmlu_df_syntactic[\"original\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_syntactic[\"rank_syntactic\"] = mmlu_df_syntactic[\"syntactic\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_syntactic.to_csv(\"../../data/result_tables/mmlu_syntactic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7fc9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 65.41\\% & 63.20\\% & 2.21pp \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 77.25\\% & 75.41\\% & 1.84pp \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 65.59\\% & 63.61\\% & 1.98pp \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 78.04\\% & 76.42\\% & 1.61pp \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 82.90\\% & 80.52\\% & 2.39pp \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 77.75\\% & 75.43\\% & 2.32pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 79.69\\% & 78.08\\% & 1.61pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 60.68\\% & 58.35\\% & 2.33pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 56.26\\% & 54.28\\% & 1.98pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 25.60\\% & 25.49\\% & 0.11pp \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 82.97\\% & 80.84\\% & 2.13pp \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 61.52\\% & 61.24\\% & 0.28pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 73.41\\% & 71.87\\% & 1.55pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 67.76\\% & 65.39\\% & 2.37pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 53.09\\% & 51.84\\% & 1.25pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 36.09\\% & 35.26\\% & 0.83pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 14.78\\% & 15.31\\% & -0.53pp \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 78.61\\% & 76.57\\% & 2.03pp \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 73.13\\% & 71.08\\% & 2.05pp \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 59.33\\% & 56.93\\% & 2.40pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 82.93\\% & 80.99\\% & 1.94pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 73.62\\% & 72.21\\% & 1.41pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 62.89\\% & 61.24\\% & 1.65pp \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in mmlu_df_syntactic.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']*100:.2f}\\\\% & {row['syntactic']*100:.2f}\\\\% & {row['score_difference']*100:.2f}pp \\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c502d7a",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dadb702a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e3dbe2382b4338bdc2ceec93ffbb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f805d580b154b00ad17055f9f45414e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d19cbaed26409181706c8e0036475a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f763e97c89b2438d8a6eda4eeae5576e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2af5be59f2e466382170b4e66067006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d328bf3998d3457fa2a863406a32586e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e8fca65e6646bb8b1f0fc6fcd2c167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83abfb3e96004d93928e6f5e833b3c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba656584340847f8ab075a00b1cdcf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc87abda48c4bda87f54649f6b44188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2991e6fd554d01ad7076009092a275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd788bef5c24ad7ba36f660e790d976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446f550a1c68435e9ce04bc348ec626a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76166cff2fa14c24bcd3df66dae4d64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772c21f1061e4a2f8f18f9014ed8479f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741148e3c79142ca9f16890a16966b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af3aabcdc834123ac784b06d0a9dece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99b1dd8998b4ea5bc7e56f339dfd189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94ee4cb8bb04b7b9d9924284b1a6908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01845dafc38f4bc5b21b83eaa65dfd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750cdd6d40f943389570d8b20d76114e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30432d388e8b41e794346dcec0a6a920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9176a760854fce93a24736aa1fe47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ba80eb9230443ca2c8e34f12320586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d250e744a7041529347a9b93e0e0eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e76b00d9fc4d91b1b6da056eb9ca82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61313e945df84ae6bb5b0abd8c101e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d73be51f70497b91f456ec31946c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a63dc123164f3ba99a8b0cba8af189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac554427a8648f8b3b12c7d3d950ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d01b4f8cf2049ff8a4780aefc68eb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56b06abce8842c687d1a4f380511eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66074a138b734a229df77fd9ab97883d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8134aeee82465e8165eebdd14d62d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e08024db104ef584f518d2fb013205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984c8a094cb54bec951d44a7d0dcb5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3d19bca2bd476d8314890e4ad41166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83e28c0b87d457495623b7471d7ab71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48642310ed444ce8cd238005e40dac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0837b6293c74f06a66754cf0b691062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c77a2efff04b589abc6c020d1166f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf6af0c956546a782153c23cce497f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15651cc33ebf4e7b8c2e839a54a7d323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dd4b859de5457993aad672f4e3d495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f56bd0e08948b7956ec947d81208b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ff3f45376e429a8e5fc2e1c6066100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa06176dadb644009242a76a687c2bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify changed rows\n",
    "squad_data = \"../../data/squad\"\n",
    "original_data_df = load_dataset(\"rajpurkar/squad\", split=\"validation\").shuffle(seed=77).select(range(1000)).to_pandas()\n",
    "with open(os.path.join(squad_data, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if (row[\"question\"] != original_data_df.iloc[index][\"question\"]) or (row[\"context\"] != original_data_df.iloc[index][\"context\"]):\n",
    "        changed_rows.append(index)\n",
    "\n",
    "# Initialize evaluators\n",
    "squad_evaluator = evaluate.load(\"squad\")\n",
    "sas_evaluator = SASEvaluator(device=ComponentDevice.from_str(\"mps\"))\n",
    "sas_evaluator.warm_up()\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "model_names = []\n",
    "original_em_scores = []\n",
    "original_f1_scores = []\n",
    "original_sas_scores = []\n",
    "syntactic_em_scores = []\n",
    "syntactic_f1_scores = []\n",
    "syntactic_sas_scores = []\n",
    "difference_em = []\n",
    "difference_f1 = []\n",
    "difference_sas = []\n",
    "for model in tqdm(os.listdir(results_dir), desc=\"Processing models\"):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(original_results_df.iterrows(), desc=\"Processing original rows\", total=len(original_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        original_em_score = statistics.mean(em_scores)\n",
    "        original_f1_score = statistics.mean(f1_scores)\n",
    "        original_sas_score = statistics.mean(sas_scores)\n",
    "        original_em_scores.append(original_em_score)\n",
    "        original_f1_scores.append(original_f1_score)\n",
    "        original_sas_scores.append(original_sas_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"syntactic\", \"syntactic.json\")) as f:\n",
    "            syntactic_results = json.load(f)\n",
    "        syntactic_results_df = pd.DataFrame(syntactic_results[\"predictions\"]).iloc[changed_rows]\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(syntactic_results_df.iterrows(), desc=\"Processing syntactic rows\", total=len(syntactic_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        syntactic_em_score = statistics.mean(em_scores)\n",
    "        syntactic_f1_score = statistics.mean(f1_scores)\n",
    "        syntactic_sas_score = statistics.mean(sas_scores)\n",
    "        syntactic_em_scores.append(syntactic_em_score)\n",
    "        syntactic_f1_scores.append(syntactic_f1_score)\n",
    "        syntactic_sas_scores.append(syntactic_sas_score)\n",
    "\n",
    "        difference_em.append(original_em_score - syntactic_em_score)\n",
    "        difference_f1.append(original_f1_score - syntactic_f1_score)\n",
    "        difference_sas.append(original_sas_score - syntactic_sas_score)\n",
    "\n",
    "squad_df_syntactic = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original_em\": original_em_scores,\n",
    "    \"original_f1\": original_f1_scores,\n",
    "    \"original_sas\": original_sas_scores,\n",
    "    \"syntactic_em\": syntactic_em_scores,\n",
    "    \"syntactic_f1\": syntactic_f1_scores,\n",
    "    \"syntactic_sas\": syntactic_sas_scores,\n",
    "    \"difference_em\": difference_em,\n",
    "    \"difference_f1\": difference_f1,\n",
    "    \"difference_sas\": difference_sas\n",
    "})\n",
    "squad_df_syntactic[\"model\"] = squad_df_syntactic[\"model\"].map(id_to_model)\n",
    "squad_df_syntactic = squad_df_syntactic.set_index(\"model\").reindex(model_order).reset_index()\n",
    "squad_df_syntactic[\"rank_original_em\"] = squad_df_syntactic[\"original_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_original_f1\"] = squad_df_syntactic[\"original_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_original_sas\"] = squad_df_syntactic[\"original_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_syntactic_em\"] = squad_df_syntactic[\"syntactic_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_syntactic_f1\"] = squad_df_syntactic[\"syntactic_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_syntactic_sas\"] = squad_df_syntactic[\"syntactic_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic.to_csv(\"../../data/result_tables/squad_syntactic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a93633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 66.43\\% & 63.23\\% & 3.19pp & 83.67\\% & 81.06\\% & 2.61pp & 90.98\\% & 89.27\\% & 1.71pp\\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 75.08\\% & 72.09\\% & 2.99pp & 89.44\\% & 87.00\\% & 2.44pp & 93.68\\% & 92.30\\% & 1.38pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 76.73\\% & 73.22\\% & 3.50pp & 89.56\\% & 86.53\\% & 3.03pp & 93.98\\% & 92.51\\% & 1.47pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 77.03\\% & 75.08\\% & 1.96pp & 90.59\\% & 88.32\\% & 2.28pp & 94.36\\% & 93.11\\% & 1.25pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 71.78\\% & 70.03\\% & 1.75pp & 87.18\\% & 85.20\\% & 1.98pp & 93.37\\% & 92.44\\% & 0.93pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 70.75\\% & 68.90\\% & 1.85pp & 87.09\\% & 85.11\\% & 1.98pp & 92.36\\% & 92.05\\% & 0.31pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 82.18\\% & 79.20\\% & 2.99pp & 92.50\\% & 90.23\\% & 2.26pp & 95.86\\% & 94.43\\% & 1.42pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 72.19\\% & 69.62\\% & 2.57pp & 86.51\\% & 84.45\\% & 2.06pp & 91.95\\% & 90.70\\% & 1.25pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 75.28\\% & 73.53\\% & 1.75pp & 87.17\\% & 84.54\\% & 2.63pp & 92.66\\% & 90.98\\% & 1.69pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 57.26\\% & 54.69\\% & 2.57pp & 71.21\\% & 67.30\\% & 3.91pp & 84.17\\% & 81.60\\% & 2.58pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 86.41\\% & 83.52\\% & 2.88pp & 94.21\\% & 91.94\\% & 2.27pp & 96.76\\% & 95.57\\% & 1.18pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 85.58\\% & 82.49\\% & 3.09pp & 93.48\\% & 90.67\\% & 2.81pp & 96.16\\% & 94.43\\% & 1.73pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 76.21\\% & 73.22\\% & 2.99pp & 90.08\\% & 87.25\\% & 2.84pp & 93.90\\% & 92.16\\% & 1.74pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 81.77\\% & 77.55\\% & 4.22pp & 91.43\\% & 88.40\\% & 3.03pp & 95.08\\% & 93.25\\% & 1.83pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 78.89\\% & 75.28\\% & 3.60pp & 89.61\\% & 86.34\\% & 3.27pp & 93.62\\% & 91.51\\% & 2.11pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 64.98\\% & 59.63\\% & 5.36pp & 77.68\\% & 72.36\\% & 5.31pp & 87.28\\% & 84.16\\% & 3.12pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 21.11\\% & 17.30\\% & 3.81pp & 36.16\\% & 30.40\\% & 5.76pp & 61.20\\% & 56.75\\% & 4.46pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 86.92\\% & 85.17\\% & 1.75pp & 93.31\\% & 91.63\\% & 1.68pp & 96.49\\% & 95.27\\% & 1.22pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 84.24\\% & 82.39\\% & 1.85pp & 92.91\\% & 91.03\\% & 1.88pp & 95.80\\% & 94.62\\% & 1.18pp\\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 80.02\\% & 79.40\\% & 0.62pp & 89.09\\% & 87.23\\% & 1.86pp & 93.47\\% & 92.34\\% & 1.13pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 70.75\\% & 69.21\\% & 1.54pp & 85.63\\% & 83.80\\% & 1.82pp & 91.96\\% & 90.53\\% & 1.43pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 76.00\\% & 73.74\\% & 2.27pp & 88.73\\% & 86.47\\% & 2.25pp & 93.70\\% & 92.37\\% & 1.33pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 69.10\\% & 67.46\\% & 1.65pp & 85.01\\% & 82.29\\% & 2.71pp & 91.38\\% & 89.08\\% & 2.30pp\\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in squad_df_syntactic.iterrows():\n",
    "    print(f\"{row['model']} & {row['original_em']:.2f}\\\\% & {row['syntactic_em']:.2f}\\\\% & {row['difference_em']:.2f}pp & {row['original_f1']:.2f}\\\\% & {row['syntactic_f1']:.2f}\\\\% & {row['difference_f1']:.2f}pp & {row['original_sas']*100:.2f}\\\\% & {row['syntactic_sas']*100:.2f}\\\\% & {row['difference_sas']*100:.2f}pp\\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0bb7e",
   "metadata": {},
   "source": [
    "## AMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7d17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for each model\n",
    "model_names = []\n",
    "amega_scores_original = []\n",
    "amega_scores_syntactic = []\n",
    "score_differences = []\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"original_v3.json\")) as f:\n",
    "            original_amega_data = json.load(f)\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"syntactic\", \"syntactic_v3.json\")) as f:\n",
    "            syntactic_amega_data = json.load(f)\n",
    "\n",
    "        model_names.append(original_amega_data[\"model\"])\n",
    "        original_score = original_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        syntactic_score = syntactic_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        amega_scores_original.append(original_score)\n",
    "        amega_scores_syntactic.append(syntactic_score)\n",
    "        score_differences.append(original_score - syntactic_score)\n",
    "\n",
    "amega_df_syntactic = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": amega_scores_original,\n",
    "    \"syntactic\": amega_scores_syntactic,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "amega_df_syntactic[\"model\"] = amega_df_syntactic[\"model\"].map(id_to_model)\n",
    "amega_df_syntactic = amega_df_syntactic.set_index(\"model\").reindex(model_order).reset_index()\n",
    "amega_df_syntactic[\"rank_original\"] = amega_df_syntactic[\"original\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_syntactic[\"rank_syntactic\"] = amega_df_syntactic[\"syntactic\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_syntactic.to_csv(\"../../data/result_tables/amega_syntactic.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "431f1a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 37.44 & 36.83 & 0.61 \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 39.64 & 38.08 & 1.55 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 34.12 & 33.54 & 0.59 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 35.99 & 35.07 & 0.93 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 39.83 & 39.17 & 0.66 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 37.74 & 37.72 & 0.03 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 32.70 & 32.31 & 0.39 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 29.80 & 29.80 & -0.00 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 26.58 & 27.35 & -0.77 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 22.19 & 21.83 & 0.36 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 38.05 & 36.92 & 1.13 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 36.04 & 34.49 & 1.55 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 35.40 & 35.46 & -0.06 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 35.14 & 35.84 & -0.70 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 32.72 & 32.15 & 0.58 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 26.89 & 26.02 & 0.87 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 15.71 & 14.44 & 1.27 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 34.85 & 33.76 & 1.10 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 36.78 & 35.04 & 1.74 \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 30.79 & 29.87 & 0.92 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 37.27 & 37.21 & 0.05 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 36.65 & 36.88 & -0.23 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 34.22 & 34.35 & -0.13 \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in amega_df_syntactic.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']:.2f} & {row['syntactic']:.2f} & {row['score_difference']:.2f} \\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216deac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
