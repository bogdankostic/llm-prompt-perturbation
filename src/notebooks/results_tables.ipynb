{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89ac662",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_model = {\n",
    "    \"openai/gpt-oss-120b\": \"GPT-OSS-120b\",\n",
    "    \"gpt-5-mini-2025-08-07\": \"GPT-5-mini\",\n",
    "    \"gemini-2.5-flash\": \"gemini-2.5-flash\",\n",
    "    \"openai/gpt-oss-20b\": \"GPT-OSS-20b\",\n",
    "    \"Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\": \"Qwen3-235B-A22B-Instruct-2507\",\n",
    "    \"gpt-5-nano-2025-08-07\": \"GPT-5-Nano\",\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\": \"Qwen3-30B-A3B-Instruct-2507\",\n",
    "    \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\": \"Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    \"gemini-2.5-flash-lite\": \"gemini-2.5-flash-lite\",\n",
    "    \"gpt-4.1-mini-2025-04-14\": \"GPT-4.1-mini\",\n",
    "    \"google/gemma-3-27b-it\": \"gemma-3-27b-it\",\n",
    "    \"mistralai/Mistral-Large-Instruct-2411\": \"Mistral-Large-Instruct-2411\",\n",
    "    \"google/gemma-3-12b-it\": \"gemma-3-12b-it\",\n",
    "    \"gpt-4.1-nano-2025-04-14\": \"GPT-4.1-Nano\",\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\": \"Qwen3-4B-Instruct-2507\",\n",
    "    \"RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8\": \"Llama-3.3-70B-Instruct\",\n",
    "    \"google/gemma-3-4b-it\": \"gemma-3-4b-it\",\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\": \"Ministral-8B-Instruct-2410\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\": \"Llama-3.1-8B-Instruct\",\n",
    "    \"google/gemma-3-1b-it\": \"gemma-3-1b-it\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": \"Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": \"Llama-3.2-1B-Instruct\",\n",
    "    \"google/gemma-3-270m-it\": \"gemma-3-270m-it\",\n",
    "}\n",
    "\n",
    "model_order = [\n",
    "    \"GPT-5-Nano\",\n",
    "    \"GPT-5-mini\",\n",
    "    \"GPT-4.1-Nano\",\n",
    "    \"GPT-4.1-mini\",\n",
    "    \"GPT-OSS-120b\",\n",
    "    \"GPT-OSS-20b\",\n",
    "    \"Llama-3.3-70B-Instruct\",\n",
    "    \"Llama-3.1-8B-Instruct\",\n",
    "    \"Llama-3.2-3B-Instruct\",\n",
    "    \"Llama-3.2-1B-Instruct\",\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "    \"gemma-3-27b-it\",\n",
    "    \"gemma-3-12b-it\",\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-1b-it\",\n",
    "    \"gemma-3-270m-it\",\n",
    "    \"Mistral-Large-Instruct-2411\",\n",
    "    \"Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    \"Ministral-8B-Instruct-2410\",\n",
    "    \"Qwen3-235B-A22B-Instruct-2507\",\n",
    "    \"Qwen3-30B-A3B-Instruct-2507\",\n",
    "    \"Qwen3-4B-Instruct-2507\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ea34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "def update_amega_metrics(amega_experiment_file, amega_criteria_df):\n",
    "    \"\"\"\n",
    "    Compute per-case scores and mean score for AMEGA predictions and write them back to the JSON file.\n",
    "    \"\"\"\n",
    "    # Read source JSON\n",
    "    with open(amega_experiment_file, \"r\") as f:\n",
    "        amega_data = json.load(f)\n",
    "    majority_votes = amega_data[\"predictions\"][\"majority_vote\"]\n",
    "\n",
    "    # Compute case scores\n",
    "    current_case_id = None\n",
    "    current_case_score = 0.0\n",
    "    case_scores = []\n",
    "\n",
    "    for (_, criterion_row), criterion_met in zip(amega_criteria_df.iterrows(), chain.from_iterable(majority_votes)):\n",
    "        row_case_id = criterion_row[\"case_id\"]\n",
    "        row_score_possible = criterion_row[\"criteria_score_possible\"]\n",
    "\n",
    "        if current_case_id is None:\n",
    "            current_case_id = row_case_id\n",
    "\n",
    "        # New case encountered: close out the previous one\n",
    "        if row_case_id != current_case_id:\n",
    "            case_scores.append(current_case_score)\n",
    "            current_case_id = row_case_id\n",
    "            current_case_score = 0.0\n",
    "\n",
    "        if criterion_met:\n",
    "            current_case_score += row_score_possible\n",
    "\n",
    "    # Append the final case's score\n",
    "    if current_case_id is not None:\n",
    "        case_scores.append(current_case_score)\n",
    "\n",
    "    mean_score = mean(case_scores)\n",
    "\n",
    "    # Write back into JSON\n",
    "    amega_data.setdefault(\"metrics\", {})\n",
    "    amega_data[\"metrics\"][\"case_scores\"] = case_scores\n",
    "    amega_data[\"metrics\"][\"mean_score\"] = mean_score\n",
    "\n",
    "    with open(amega_experiment_file, \"w\") as f:\n",
    "        json.dump(amega_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29c82c",
   "metadata": {},
   "source": [
    "# Lexical Perturbations\n",
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42745dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Identify changed rows\n",
    "mmlu_data_dir = \"../../data/mmlu\"\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"original.json\")) as f:\n",
    "    original_data = json.load(f)\n",
    "original_data_df = pd.DataFrame(original_data[\"data\"])\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if row[\"question\"] != original_data_df.iloc[index][\"question\"] or row[\"choices\"] != original_data_df.iloc[index][\"choices\"]:\n",
    "        changed_rows.append(index)\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "results_dir = \"../../results\"\n",
    "\n",
    "model_names = []\n",
    "original_scores = []\n",
    "lexical_scores = []\n",
    "score_differences = []\n",
    "for model_dir in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model_dir)):\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        original_results_df[\"correct\"] = original_results_df[\"prediction\"] == original_results_df[\"answer\"]\n",
    "        original_score = original_results_df[\"correct\"].mean()\n",
    "        original_scores.append(original_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"lexical\", \"llm_synonym.json\")) as f:\n",
    "            lexical_results = json.load(f)\n",
    "        lexical_results_df = pd.DataFrame(lexical_results[\"predictions\"]).iloc[changed_rows]\n",
    "        lexical_results_df[\"correct\"] = lexical_results_df[\"prediction\"] == lexical_results_df[\"answer\"]\n",
    "        lexical_score = lexical_results_df[\"correct\"].mean()\n",
    "        lexical_scores.append(lexical_score)\n",
    "\n",
    "        score_differences.append(original_score - lexical_score)\n",
    "\n",
    "mmlu_df_lexical = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": original_scores,\n",
    "    \"lexical\": lexical_scores,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "mmlu_df_lexical[\"model\"] = mmlu_df_lexical[\"model\"].map(id_to_model)\n",
    "mmlu_df_lexical = mmlu_df_lexical.set_index(\"model\").reindex(model_order).reset_index()\n",
    "mmlu_df_lexical[\"rank_original\"] = mmlu_df_lexical[\"original\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_lexical[\"rank_lexical\"] = mmlu_df_lexical[\"lexical\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_lexical.to_csv(\"../../data/result_tables/mmlu_lexical.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a8a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 69.62\\% & 59.44\\% & 10.18pp \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 80.07\\% & 70.78\\% & 9.29pp \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 69.97\\% & 62.16\\% & 7.81pp \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 80.76\\% & 72.51\\% & 8.25pp \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 86.20\\% & 76.48\\% & 9.71pp \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 81.46\\% & 71.87\\% & 9.59pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 80.49\\% & 71.22\\% & 9.27pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 62.86\\% & 54.78\\% & 8.08pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 58.05\\% & 51.03\\% & 7.02pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 25.71\\% & 24.87\\% & 0.85pp \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 84.97\\% & 76.19\\% & 8.78pp \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 63.49\\% & 54.91\\% & 8.59pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 76.63\\% & 67.17\\% & 9.46pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 71.19\\% & 62.52\\% & 8.68pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 57.25\\% & 51.56\\% & 5.69pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 38.83\\% & 36.12\\% & 2.71pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 13.38\\% & 13.70\\% & -0.32pp \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 80.06\\% & 70.71\\% & 9.35pp \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 76.66\\% & 67.33\\% & 9.33pp \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 61.70\\% & 54.11\\% & 7.59pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 85.59\\% & 76.15\\% & 9.44pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 77.32\\% & 68.88\\% & 8.44pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 66.05\\% & 56.34\\% & 9.71pp \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in mmlu_df_lexical.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']*100:.2f}\\\\% & {row['lexical']*100:.2f}\\\\% & {row['score_difference']*100:.2f}pp \\\\\\\\\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089f121",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67afc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa5d82a900d43c9b04e278e898866b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e792506b32114541a96ae339625db936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546bef3d69994910901559ed4098cd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bcd21fa808483f97b0f39d67a9c23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02585a389ca94827a23810f6dbff909d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da774f04db4a4278a0a43d970bc71acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b5144bc85a4e4882479d3947000aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633deda485bf4f8bacdf48987966fadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020bb8c0b34a4be8a008a24b534e55a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9b66c24b9441f0a695df2192e5622d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d1994b23104f35829fc2ea69f10aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984a429ee07b464d892b63ca987a10b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354157c1f42f44a2b5a304a2ff0e5bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f91f7ba35247ceaaa278e0516e38ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3567e97e674e998d53c3971436815d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddcfbcdbf864ebf8ffe9b06d31da387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390b2b79ddc840159a059c54f46b73c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a4482066024099854aa07a81730ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712ab7b48f814983b3b7dc5cd81f2258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bceb1b8fce4970bab6433501aa2d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00f557c4c054cbd817202f8bffede23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c457640d0b04867882bd1dfa2bd4afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c10e19cbd354fd2ae9b41959137eeb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a03e5ea97ff49059367d286cbd91970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcb59526dd94f989d6efa13c6a1818c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85766f3fd7f545d5a348432ce57b10d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef193529bd9c439f9a4fc7b7a26d7d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8db6318dd0449518cf56fe83a92c8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c87b39dd2c0477f85c1806c0437ffae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d11144ae6c4490b0de303df2d2924f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e0465cdf6d48dfba980f18af88bca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc5ea2e4b2044b79774de4c699f8d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3938c88e3264efa8205b1be89eae7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdbb395ae764d108c87ac8444cf7738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe4a12b583140ea8cabf122cc1cf55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473e4d994d9e415cb56ee4ed75f56c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cf9aa6352f4ec198ef5003e544dd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e0ec12a1b24043a5569c57628950f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56a6fd649c94759a045e7e2f37786d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b721af2054ac4ef99535020eaa95f028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b84b0d493a14b91bd7916eb38ebd85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5c4dbfb6a7424fa1f1784e1e640d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384337eaa96d4c64ac76d1d6a033470b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13c0b6aaf45445babd954708f456ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5c782ac4fb48c0babe6b1b2dc685b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bc233a0ed44c51b0cd475e76d51e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bc177e941046488102c3c0d32fd0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing lexical rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "from haystack.components.evaluators import SASEvaluator\n",
    "from haystack.utils import ComponentDevice\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Identify changed rows\n",
    "squad_data = \"../../data/squad\"\n",
    "original_data_df = load_dataset(\"rajpurkar/squad\", split=\"validation\").shuffle(seed=77).select(range(1000)).to_pandas()\n",
    "with open(os.path.join(squad_data, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if (row[\"question\"] != original_data_df.iloc[index][\"question\"]) or (row[\"context\"] != original_data_df.iloc[index][\"context\"]):\n",
    "        changed_rows.append(index)\n",
    "\n",
    "# Initialize evaluators\n",
    "squad_evaluator = evaluate.load(\"squad\")\n",
    "sas_evaluator = SASEvaluator(device=ComponentDevice.from_str(\"mps\"))\n",
    "sas_evaluator.warm_up()\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "model_names = []\n",
    "original_em_scores = []\n",
    "original_f1_scores = []\n",
    "original_sas_scores = []\n",
    "lexical_em_scores = []\n",
    "lexical_f1_scores = []\n",
    "lexical_sas_scores = []\n",
    "difference_em = []\n",
    "difference_f1 = []\n",
    "difference_sas = []\n",
    "for model in tqdm(os.listdir(results_dir), desc=\"Processing models\"):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(original_results_df.iterrows(), desc=\"Processing original rows\", total=len(original_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        original_em_score = statistics.mean(em_scores)\n",
    "        original_f1_score = statistics.mean(f1_scores)\n",
    "        original_sas_score = statistics.mean(sas_scores)\n",
    "        original_em_scores.append(original_em_score)\n",
    "        original_f1_scores.append(original_f1_score)\n",
    "        original_sas_scores.append(original_sas_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"lexical\", \"llm_synonym.json\")) as f:\n",
    "            lexical_results = json.load(f)\n",
    "        lexical_results_df = pd.DataFrame(lexical_results[\"predictions\"]).iloc[changed_rows]\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(lexical_results_df.iterrows(), desc=\"Processing lexical rows\", total=len(lexical_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        lexical_em_score = statistics.mean(em_scores)\n",
    "        lexical_f1_score = statistics.mean(f1_scores)\n",
    "        lexical_sas_score = statistics.mean(sas_scores)\n",
    "        lexical_em_scores.append(lexical_em_score)\n",
    "        lexical_f1_scores.append(lexical_f1_score)\n",
    "        lexical_sas_scores.append(lexical_sas_score)\n",
    "\n",
    "        difference_em.append(original_em_score - lexical_em_score)\n",
    "        difference_f1.append(original_f1_score - lexical_f1_score)\n",
    "        difference_sas.append(original_sas_score - lexical_sas_score)\n",
    "\n",
    "squad_df_lexical = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original_em\": original_em_scores,\n",
    "    \"original_f1\": original_f1_scores,\n",
    "    \"original_sas\": original_sas_scores,\n",
    "    \"lexical_em\": lexical_em_scores,\n",
    "    \"lexical_f1\": lexical_f1_scores,\n",
    "    \"lexical_sas\": lexical_sas_scores,\n",
    "    \"difference_em\": difference_em,\n",
    "    \"difference_f1\": difference_f1,\n",
    "    \"difference_sas\": difference_sas\n",
    "})\n",
    "squad_df_lexical[\"model\"] = squad_df_lexical[\"model\"].map(id_to_model)\n",
    "squad_df_lexical = squad_df_lexical.set_index(\"model\").reindex(model_order).reset_index()\n",
    "squad_df_lexical[\"rank_original_em\"] = squad_df_lexical[\"original_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_original_f1\"] = squad_df_lexical[\"original_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_original_sas\"] = squad_df_lexical[\"original_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_lexical_em\"] = squad_df_lexical[\"lexical_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_lexical_f1\"] = squad_df_lexical[\"lexical_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical[\"rank_lexical_sas\"] = squad_df_lexical[\"lexical_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_lexical.to_csv(\"../../data/result_tables/squad_lexical.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5658a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 66.43\\% & 62.92\\% & 3.50pp & 83.67\\% & 80.11\\% & 3.55pp & 90.98\\% & 88.81\\% & 2.17pp\\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 75.08\\% & 71.06\\% & 4.02pp & 89.44\\% & 85.99\\% & 3.45pp & 93.68\\% & 92.07\\% & 1.61pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 76.73\\% & 71.47\\% & 5.25pp & 89.56\\% & 85.60\\% & 3.96pp & 93.98\\% & 91.91\\% & 2.07pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 77.03\\% & 72.50\\% & 4.53pp & 90.59\\% & 86.85\\% & 3.74pp & 94.36\\% & 92.51\\% & 1.85pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 71.78\\% & 67.35\\% & 4.43pp & 87.18\\% & 83.73\\% & 3.44pp & 93.37\\% & 91.48\\% & 1.89pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 70.75\\% & 65.91\\% & 4.84pp & 87.09\\% & 83.05\\% & 4.04pp & 92.36\\% & 90.54\\% & 1.83pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 82.18\\% & 77.14\\% & 5.05pp & 92.50\\% & 88.96\\% & 3.53pp & 95.86\\% & 94.00\\% & 1.86pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 72.19\\% & 66.94\\% & 5.25pp & 86.51\\% & 83.16\\% & 3.35pp & 91.95\\% & 90.36\\% & 1.60pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 75.28\\% & 71.88\\% & 3.40pp & 87.17\\% & 84.13\\% & 3.05pp & 92.66\\% & 91.15\\% & 1.51pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 57.26\\% & 53.04\\% & 4.22pp & 71.21\\% & 67.11\\% & 4.09pp & 84.17\\% & 81.73\\% & 2.45pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 86.41\\% & 83.52\\% & 2.88pp & 94.21\\% & 91.68\\% & 2.52pp & 96.76\\% & 95.57\\% & 1.19pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 85.58\\% & 80.33\\% & 5.25pp & 93.48\\% & 89.56\\% & 3.92pp & 96.16\\% & 94.39\\% & 1.77pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 76.21\\% & 71.88\\% & 4.33pp & 90.08\\% & 87.20\\% & 2.88pp & 93.90\\% & 92.44\\% & 1.46pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 81.77\\% & 77.55\\% & 4.22pp & 91.43\\% & 88.58\\% & 2.85pp & 95.08\\% & 93.62\\% & 1.46pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 78.89\\% & 74.77\\% & 4.12pp & 89.61\\% & 86.59\\% & 3.01pp & 93.62\\% & 92.18\\% & 1.45pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 64.98\\% & 60.76\\% & 4.22pp & 77.68\\% & 73.26\\% & 4.42pp & 87.28\\% & 85.20\\% & 2.08pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 21.11\\% & 20.19\\% & 0.93pp & 36.16\\% & 33.52\\% & 2.64pp & 61.20\\% & 59.21\\% & 1.99pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 86.92\\% & 83.11\\% & 3.81pp & 93.31\\% & 90.50\\% & 2.80pp & 96.49\\% & 95.48\\% & 1.01pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 84.24\\% & 79.20\\% & 5.05pp & 92.91\\% & 89.48\\% & 3.43pp & 95.80\\% & 93.91\\% & 1.89pp\\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 80.02\\% & 75.80\\% & 4.22pp & 89.09\\% & 85.36\\% & 3.73pp & 93.47\\% & 91.70\\% & 1.77pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 70.75\\% & 68.69\\% & 2.06pp & 85.63\\% & 83.74\\% & 1.89pp & 91.96\\% & 90.79\\% & 1.18pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 76.00\\% & 71.58\\% & 4.43pp & 88.73\\% & 85.73\\% & 3.00pp & 93.70\\% & 91.97\\% & 1.73pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 69.10\\% & 63.34\\% & 5.77pp & 85.01\\% & 80.61\\% & 4.40pp & 91.38\\% & 88.74\\% & 2.64pp\\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in squad_df_lexical.iterrows():\n",
    "    print(f\"{row['model']} & {row['original_em']:.2f}\\\\% & {row['lexical_em']:.2f}\\\\% & {row['difference_em']:.2f}pp & {row['original_f1']:.2f}\\\\% & {row['lexical_f1']:.2f}\\\\% & {row['difference_f1']:.2f}pp & {row['original_sas']*100:.2f}\\\\% & {row['lexical_sas']*100:.2f}\\\\% & {row['difference_sas']*100:.2f}pp\\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9fa84",
   "metadata": {},
   "source": [
    "## AMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AMEGA score based on weighted criteria\n",
    "amega_criteria_df = pd.read_csv(\"../../AMEGA-benchmark/data/criteria.csv\", sep=\";\", decimal=\",\")\n",
    "\n",
    "for item in os.listdir(results_dir):\n",
    "    current_dir = os.path.join(results_dir, item)\n",
    "    if os.path.isdir(current_dir):\n",
    "        original_amega_file = os.path.join(current_dir, \"amega/original_v3.json\")\n",
    "        syntactic_variation_file = os.path.join(current_dir, \"amega/syntactic/syntactic_v3.json\")\n",
    "        lexical_variation_file = os.path.join(current_dir, \"amega/lexical/llm_synonym_v3.json\")\n",
    "        \n",
    "        update_amega_metrics(original_amega_file, amega_criteria_df)\n",
    "        update_amega_metrics(syntactic_variation_file, amega_criteria_df)\n",
    "        update_amega_metrics(lexical_variation_file, amega_criteria_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d28391c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for each model\n",
    "model_names = []\n",
    "amega_scores_original = []\n",
    "amega_scores_lexical = []\n",
    "score_differences = []\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"original_v3.json\")) as f:\n",
    "            original_amega_data = json.load(f)\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"lexical\", \"llm_synonym_v3.json\")) as f:\n",
    "            lexical_amega_data = json.load(f)\n",
    "\n",
    "        model_names.append(original_amega_data[\"model\"])\n",
    "        original_score = original_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        lexical_score = lexical_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        amega_scores_original.append(original_score)\n",
    "        amega_scores_lexical.append(lexical_score)\n",
    "        score_differences.append(original_score - lexical_score)\n",
    "\n",
    "amega_df_lexical = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": amega_scores_original,\n",
    "    \"lexical\": amega_scores_lexical,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "amega_df_lexical[\"model\"] = amega_df_lexical[\"model\"].map(id_to_model)\n",
    "amega_df_lexical = amega_df_lexical.set_index(\"model\").reindex(model_order).reset_index()\n",
    "amega_df_lexical[\"rank_original\"] = amega_df_lexical[\"original\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_lexical[\"rank_lexical\"] = amega_df_lexical[\"lexical\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_lexical.to_csv(\"../../data/result_tables/amega_lexical.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "403d0fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 37.44 & 35.51 & 1.93 \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 39.64 & 37.49 & 2.14 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 34.12 & 33.41 & 0.72 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 35.99 & 35.65 & 0.34 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 39.83 & 39.36 & 0.47 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 37.74 & 36.07 & 1.68 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 32.70 & 32.21 & 0.49 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 29.80 & 28.35 & 1.45 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 26.58 & 25.28 & 1.30 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 22.19 & 19.71 & 2.48 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 38.05 & 37.94 & 0.11 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 36.04 & 34.96 & 1.07 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 35.40 & 34.20 & 1.20 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 35.14 & 34.41 & 0.73 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 32.72 & 31.51 & 1.21 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 26.89 & 26.17 & 0.72 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 15.71 & 11.70 & 4.01 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 34.85 & 32.47 & 2.39 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 36.78 & 35.27 & 1.51 \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 30.79 & 29.30 & 1.50 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 37.27 & 37.02 & 0.25 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 36.65 & 36.58 & 0.07 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 34.22 & 33.20 & 1.03 \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in amega_df_lexical.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']:.2f} & {row['lexical']:.2f} & {row['score_difference']:.2f} \\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0082de",
   "metadata": {},
   "source": [
    "# Syntactic Perturbations\n",
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd905388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify changed rows\n",
    "mmlu_data_dir = \"../../data/mmlu\"\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"original.json\")) as f:\n",
    "    original_data = json.load(f)\n",
    "original_data_df = pd.DataFrame(original_data[\"data\"])\n",
    "\n",
    "with open(os.path.join(mmlu_data_dir, \"syntactic\", \"syntactic_perturbation.json\")) as f:\n",
    "    syntactic_data = json.load(f)\n",
    "syntactic_data_df = pd.DataFrame(syntactic_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in syntactic_data_df.iterrows():\n",
    "    if row[\"question\"] != original_data_df.iloc[index][\"question\"] or row[\"choices\"] != original_data_df.iloc[index][\"choices\"]:\n",
    "        changed_rows.append(index)\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "results_dir = \"../../results\"\n",
    "\n",
    "model_names = []\n",
    "original_scores = []\n",
    "syntactic_scores = []\n",
    "score_differences = []\n",
    "for model_dir in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model_dir)):\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        original_results_df[\"correct\"] = original_results_df[\"prediction\"] == original_results_df[\"answer\"]\n",
    "        original_score = original_results_df[\"correct\"].mean()\n",
    "        original_scores.append(original_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model_dir, \"mmlu\", \"syntactic\", \"syntactic.json\")) as f:\n",
    "            syntactic_results = json.load(f)\n",
    "        syntactic_results_df = pd.DataFrame(syntactic_results[\"predictions\"]).iloc[changed_rows]\n",
    "        syntactic_results_df[\"correct\"] = syntactic_results_df[\"prediction\"] == syntactic_results_df[\"answer\"]\n",
    "        syntactic_score = syntactic_results_df[\"correct\"].mean()\n",
    "        syntactic_scores.append(syntactic_score)\n",
    "\n",
    "        score_differences.append(original_score - syntactic_score)\n",
    "\n",
    "mmlu_df_syntactic = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": original_scores,\n",
    "    \"syntactic\": syntactic_scores,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "mmlu_df_syntactic[\"model\"] = mmlu_df_syntactic[\"model\"].map(id_to_model)\n",
    "mmlu_df_syntactic = mmlu_df_syntactic.set_index(\"model\").reindex(model_order).reset_index()\n",
    "mmlu_df_syntactic[\"rank_original\"] = mmlu_df_syntactic[\"original\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_syntactic[\"rank_syntactic\"] = mmlu_df_syntactic[\"syntactic\"].rank(method=\"min\", ascending=False)\n",
    "mmlu_df_syntactic.to_csv(\"../../data/result_tables/mmlu_syntactic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c7fc9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 65.41\\% & 63.20\\% & 2.21pp \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 77.25\\% & 75.41\\% & 1.84pp \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 65.59\\% & 63.61\\% & 1.98pp \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 78.04\\% & 76.42\\% & 1.61pp \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 82.90\\% & 80.52\\% & 2.39pp \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 77.75\\% & 75.43\\% & 2.32pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 79.69\\% & 78.08\\% & 1.61pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 60.68\\% & 58.35\\% & 2.33pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 56.26\\% & 54.28\\% & 1.98pp \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 25.60\\% & 25.49\\% & 0.11pp \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 82.97\\% & 80.84\\% & 2.13pp \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 61.52\\% & 61.24\\% & 0.28pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 73.41\\% & 71.87\\% & 1.55pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 67.76\\% & 65.39\\% & 2.37pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 53.09\\% & 51.84\\% & 1.25pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 36.09\\% & 35.26\\% & 0.83pp \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 14.78\\% & 15.31\\% & -0.53pp \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 78.61\\% & 76.57\\% & 2.03pp \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 73.13\\% & 71.08\\% & 2.05pp \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 59.33\\% & 56.93\\% & 2.40pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 82.93\\% & 80.99\\% & 1.94pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 73.62\\% & 72.21\\% & 1.41pp \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 62.89\\% & 61.24\\% & 1.65pp \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in mmlu_df_syntactic.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']*100:.2f}\\\\% & {row['syntactic']*100:.2f}\\\\% & {row['score_difference']*100:.2f}pp \\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c502d7a",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb702a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193352e2a1ac421a948b5d94de341bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d6bc686b454e69be06573c5a9130ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d8f33d601442e59eadf83dc57a433e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e8eccafa604572a52c73ccba1c0fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e100fa8ced4414bc7bffdaa5b7af9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15d059c0d0c4a4ba30fe8c3780b2b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfd5ea7060945a9a043589d088fe191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8262abe3c8f41868ea9843075580da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11f6d89e506414fa1fcccec7282e29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48241e06225f4480aa610de0a990b51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fe155f5f784e3d9a079164f862a21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc84077a53d49a5a9d9b4dc0074f0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2575703643412fb4bc84bd8da23f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087a3e71a70f42d38e78758f2970ec5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213cecf35a974d039d597b09a452443b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2c9387e70b4af4b6d7707961baaf1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c6bae3d77c4edd8190d18ccd8aca7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb244802988a4750a0692266caa37e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b65475e49d407998c39eb8763daf01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfd098817c3425ebdc7e0ace760cda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ec42d880dc4d05a353ba1382ce8465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d70d41588d45a38ac95065f14ca9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a0be13b2584b68824f1f289c77c3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d2cdd686b54913a07be54b95647a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e197952b6d40609ccb1a9009cec506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4992694b814f7992661c20f302f493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f5fa3fbcc54cd8908be483a8df2084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d1a1e0daf54781addc7be3679a9631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1b76f8b806414db5819d1241397250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f6bf568cfc4317bdece4e521833e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0b6bc8cf56492485f9c3b2a91f4759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b330867319a437aaaf8e4fcbf7bb97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e274da0e01114c6099d929b7382ed894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9cd50b3623442d281b49710997df111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8ef388607f42e09f073033f52a6a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba8245b19aa4e07897265b1fd92261d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d2d45f027d4e2a8b84b3713fe73318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20094e46e545472d984d1eb3bd220b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8efa3571854b01a7adf3058a552bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004d0d915a894bcd842d077644582430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddd68fdb2484be092ac7172d2652a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2332b835e1dd4708aecc78cd0cce6691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73dadd1d91be43d6ba8034a50c9f8389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f0da651bf24fefb68e30a6ff9c61b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b74427a86a4541ac5cc3dac4b6b12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f961ea31c4c49cd87bec0c3fdc598cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing original rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d66992199e416182323bbb6aced32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing syntactic rows:   0%|          | 0/971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Identify changed rows\n",
    "squad_data = \"../../data/squad\"\n",
    "original_data_df = load_dataset(\"rajpurkar/squad\", split=\"validation\").shuffle(seed=77).select(range(1000)).to_pandas()\n",
    "with open(os.path.join(squad_data, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if (row[\"question\"] != original_data_df.iloc[index][\"question\"]) or (row[\"context\"] != original_data_df.iloc[index][\"context\"]):\n",
    "        changed_rows.append(index)\n",
    "\n",
    "# Initialize evaluators\n",
    "squad_evaluator = evaluate.load(\"squad\")\n",
    "sas_evaluator = SASEvaluator(device=ComponentDevice.from_str(\"mps\"))\n",
    "sas_evaluator.warm_up()\n",
    "\n",
    "\n",
    "# Extract results for each model\n",
    "model_names = []\n",
    "original_em_scores = []\n",
    "original_f1_scores = []\n",
    "original_sas_scores = []\n",
    "syntactic_em_scores = []\n",
    "syntactic_f1_scores = []\n",
    "syntactic_sas_scores = []\n",
    "difference_em = []\n",
    "difference_f1 = []\n",
    "difference_sas = []\n",
    "for model in tqdm(os.listdir(results_dir), desc=\"Processing models\"):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(original_results_df.iterrows(), desc=\"Processing original rows\", total=len(original_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        original_em_score = statistics.mean(em_scores)\n",
    "        original_f1_score = statistics.mean(f1_scores)\n",
    "        original_sas_score = statistics.mean(sas_scores)\n",
    "        original_em_scores.append(original_em_score)\n",
    "        original_f1_scores.append(original_f1_score)\n",
    "        original_sas_scores.append(original_sas_score)\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"syntactic\", \"syntactic.json\")) as f:\n",
    "            syntactic_results = json.load(f)\n",
    "        syntactic_results_df = pd.DataFrame(syntactic_results[\"predictions\"]).iloc[changed_rows]\n",
    "        \n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        sas_scores = []\n",
    "        for idx, row in tqdm(syntactic_results_df.iterrows(), desc=\"Processing syntactic rows\", total=len(syntactic_results_df)):\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            sas_eval = sas_evaluator.run(row[\"answers\"], [row[\"prediction\"]] * len(row[\"answers\"]))\n",
    "            em_scores.append(squad_eval[\"exact_match\"])\n",
    "            f1_scores.append(squad_eval[\"f1\"])\n",
    "            sas_scores.append(max(sas_eval[\"individual_scores\"]))\n",
    "\n",
    "        syntactic_em_score = statistics.mean(em_scores)\n",
    "        syntactic_f1_score = statistics.mean(f1_scores)\n",
    "        syntactic_sas_score = statistics.mean(sas_scores)\n",
    "        syntactic_em_scores.append(syntactic_em_score)\n",
    "        syntactic_f1_scores.append(syntactic_f1_score)\n",
    "        syntactic_sas_scores.append(syntactic_sas_score)\n",
    "\n",
    "        difference_em.append(original_em_score - syntactic_em_score)\n",
    "        difference_f1.append(original_f1_score - syntactic_f1_score)\n",
    "        difference_sas.append(original_sas_score - syntactic_sas_score)\n",
    "\n",
    "squad_df_syntactic = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original_em\": original_em_scores,\n",
    "    \"original_f1\": original_f1_scores,\n",
    "    \"original_sas\": original_sas_scores,\n",
    "    \"syntactic_em\": syntactic_em_scores,\n",
    "    \"syntactic_f1\": syntactic_f1_scores,\n",
    "    \"syntactic_sas\": syntactic_sas_scores,\n",
    "    \"difference_em\": difference_em,\n",
    "    \"difference_f1\": difference_f1,\n",
    "    \"difference_sas\": difference_sas\n",
    "})\n",
    "squad_df_syntactic[\"model\"] = squad_df_syntactic[\"model\"].map(id_to_model)\n",
    "squad_df_syntactic = squad_df_syntactic.set_index(\"model\").reindex(model_order).reset_index()\n",
    "squad_df_syntactic[\"rank_original_em\"] = squad_df_syntactic[\"original_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_original_f1\"] = squad_df_syntactic[\"original_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_original_sas\"] = squad_df_syntactic[\"original_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_syntactic_em\"] = squad_df_syntactic[\"syntactic_em\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_syntactic_f1\"] = squad_df_syntactic[\"syntactic_f1\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic[\"rank_syntactic_sas\"] = squad_df_syntactic[\"syntactic_sas\"].rank(method=\"min\", ascending=False)\n",
    "squad_df_syntactic.to_csv(\"../../data/result_tables/squad_syntactic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a93633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 66.43\\% & 63.23\\% & 3.19pp & 83.67\\% & 81.06\\% & 2.61pp & 90.98\\% & 89.27\\% & 1.71pp\\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 75.08\\% & 72.09\\% & 2.99pp & 89.44\\% & 87.00\\% & 2.44pp & 93.68\\% & 92.30\\% & 1.38pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 76.73\\% & 73.22\\% & 3.50pp & 89.56\\% & 86.53\\% & 3.03pp & 93.98\\% & 92.51\\% & 1.47pp\\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 77.03\\% & 75.08\\% & 1.96pp & 90.59\\% & 88.32\\% & 2.28pp & 94.36\\% & 93.11\\% & 1.25pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 71.78\\% & 70.03\\% & 1.75pp & 87.18\\% & 85.20\\% & 1.98pp & 93.37\\% & 92.44\\% & 0.93pp\\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 70.75\\% & 68.90\\% & 1.85pp & 87.09\\% & 85.11\\% & 1.98pp & 92.36\\% & 92.05\\% & 0.31pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 82.18\\% & 79.20\\% & 2.99pp & 92.50\\% & 90.23\\% & 2.26pp & 95.86\\% & 94.43\\% & 1.42pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 72.19\\% & 69.62\\% & 2.57pp & 86.51\\% & 84.45\\% & 2.06pp & 91.95\\% & 90.70\\% & 1.25pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 75.28\\% & 73.53\\% & 1.75pp & 87.17\\% & 84.54\\% & 2.63pp & 92.66\\% & 90.98\\% & 1.69pp\\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 57.26\\% & 54.69\\% & 2.57pp & 71.21\\% & 67.30\\% & 3.91pp & 84.17\\% & 81.60\\% & 2.58pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 86.41\\% & 83.52\\% & 2.88pp & 94.21\\% & 91.94\\% & 2.27pp & 96.76\\% & 95.57\\% & 1.18pp\\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 85.58\\% & 82.49\\% & 3.09pp & 93.48\\% & 90.67\\% & 2.81pp & 96.16\\% & 94.43\\% & 1.73pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 76.21\\% & 73.22\\% & 2.99pp & 90.08\\% & 87.25\\% & 2.84pp & 93.90\\% & 92.16\\% & 1.74pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 81.77\\% & 77.55\\% & 4.22pp & 91.43\\% & 88.40\\% & 3.03pp & 95.08\\% & 93.25\\% & 1.83pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 78.89\\% & 75.28\\% & 3.60pp & 89.61\\% & 86.34\\% & 3.27pp & 93.62\\% & 91.51\\% & 2.11pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 64.98\\% & 59.63\\% & 5.36pp & 77.68\\% & 72.36\\% & 5.31pp & 87.28\\% & 84.16\\% & 3.12pp\\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 21.11\\% & 17.30\\% & 3.81pp & 36.16\\% & 30.40\\% & 5.76pp & 61.20\\% & 56.75\\% & 4.46pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 86.92\\% & 85.17\\% & 1.75pp & 93.31\\% & 91.63\\% & 1.68pp & 96.49\\% & 95.27\\% & 1.22pp\\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 84.24\\% & 82.39\\% & 1.85pp & 92.91\\% & 91.03\\% & 1.88pp & 95.80\\% & 94.62\\% & 1.18pp\\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 80.02\\% & 79.40\\% & 0.62pp & 89.09\\% & 87.23\\% & 1.86pp & 93.47\\% & 92.34\\% & 1.13pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 70.75\\% & 69.21\\% & 1.54pp & 85.63\\% & 83.80\\% & 1.82pp & 91.96\\% & 90.53\\% & 1.43pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 76.00\\% & 73.74\\% & 2.27pp & 88.73\\% & 86.47\\% & 2.25pp & 93.70\\% & 92.37\\% & 1.33pp\\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 69.10\\% & 67.46\\% & 1.65pp & 85.01\\% & 82.29\\% & 2.71pp & 91.38\\% & 89.08\\% & 2.30pp\\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in squad_df_syntactic.iterrows():\n",
    "    print(f\"{row['model']} & {row['original_em']:.2f}\\\\% & {row['syntactic_em']:.2f}\\\\% & {row['difference_em']:.2f}pp & {row['original_f1']:.2f}\\\\% & {row['syntactic_f1']:.2f}\\\\% & {row['difference_f1']:.2f}pp & {row['original_sas']*100:.2f}\\\\% & {row['syntactic_sas']*100:.2f}\\\\% & {row['difference_sas']*100:.2f}pp\\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0bb7e",
   "metadata": {},
   "source": [
    "## AMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea7d17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for each model\n",
    "model_names = []\n",
    "amega_scores_original = []\n",
    "amega_scores_syntactic = []\n",
    "score_differences = []\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"original_v3.json\")) as f:\n",
    "            original_amega_data = json.load(f)\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"syntactic\", \"syntactic_v3.json\")) as f:\n",
    "            syntactic_amega_data = json.load(f)\n",
    "\n",
    "        model_names.append(original_amega_data[\"model\"])\n",
    "        original_score = original_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        syntactic_score = syntactic_amega_data[\"metrics\"][\"mean_score\"]\n",
    "        amega_scores_original.append(original_score)\n",
    "        amega_scores_syntactic.append(syntactic_score)\n",
    "        score_differences.append(original_score - syntactic_score)\n",
    "\n",
    "amega_df_syntactic = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"original\": amega_scores_original,\n",
    "    \"syntactic\": amega_scores_syntactic,\n",
    "    \"score_difference\": score_differences\n",
    "})\n",
    "amega_df_syntactic[\"model\"] = amega_df_syntactic[\"model\"].map(id_to_model)\n",
    "amega_df_syntactic = amega_df_syntactic.set_index(\"model\").reindex(model_order).reset_index()\n",
    "amega_df_syntactic[\"rank_original\"] = amega_df_syntactic[\"original\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_syntactic[\"rank_syntactic\"] = amega_df_syntactic[\"syntactic\"].rank(method=\"min\", ascending=False)\n",
    "amega_df_syntactic.to_csv(\"../../data/result_tables/amega_syntactic.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "431f1a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-5-Nano & 37.44 & 36.83 & 0.61 \\\\n\\addlinespace[3pt]\n",
      "GPT-5-mini & 39.64 & 38.08 & 1.55 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-Nano & 34.12 & 33.54 & 0.59 \\\\n\\addlinespace[3pt]\n",
      "GPT-4.1-mini & 35.99 & 35.07 & 0.93 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-120b & 39.83 & 39.17 & 0.66 \\\\n\\addlinespace[3pt]\n",
      "GPT-OSS-20b & 37.74 & 37.72 & 0.03 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.3-70B-Instruct & 32.70 & 32.31 & 0.39 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.1-8B-Instruct & 29.80 & 29.80 & -0.00 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-3B-Instruct & 26.58 & 27.35 & -0.77 \\\\n\\addlinespace[3pt]\n",
      "Llama-3.2-1B-Instruct & 22.19 & 21.83 & 0.36 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash & 38.05 & 36.92 & 1.13 \\\\n\\addlinespace[3pt]\n",
      "gemini-2.5-flash-lite & 36.04 & 34.49 & 1.55 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-27b-it & 35.40 & 35.46 & -0.06 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-12b-it & 35.14 & 35.84 & -0.70 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-4b-it & 32.72 & 32.15 & 0.58 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-1b-it & 26.89 & 26.02 & 0.87 \\\\n\\addlinespace[3pt]\n",
      "gemma-3-270m-it & 15.71 & 14.44 & 1.27 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Large-Instruct-2411 & 34.85 & 33.76 & 1.10 \\\\n\\addlinespace[3pt]\n",
      "Mistral-Small-3.2-24B-Instruct-2506 & 36.78 & 35.04 & 1.74 \\\\n\\addlinespace[3pt]\n",
      "Ministral-8B-Instruct-2410 & 30.79 & 29.87 & 0.92 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-235B-A22B-Instruct-2507 & 37.27 & 37.21 & 0.05 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-30B-A3B-Instruct-2507 & 36.65 & 36.88 & -0.23 \\\\n\\addlinespace[3pt]\n",
      "Qwen3-4B-Instruct-2507 & 34.22 & 34.35 & -0.13 \\\\n\\addlinespace[3pt]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in amega_df_syntactic.iterrows():\n",
    "    print(f\"{row['model']} & {row['original']:.2f} & {row['syntactic']:.2f} & {row['score_difference']:.2f} \\\\\\\\n\\\\addlinespace[3pt]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216deac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
