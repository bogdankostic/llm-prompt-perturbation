{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79a5e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_model = {\n",
    "    \"openai/gpt-oss-120b\": \"GPT-OSS-120b\",\n",
    "    \"gpt-5-mini-2025-08-07\": \"GPT-5-mini\",\n",
    "    \"gemini-2.5-flash\": \"gemini-2.5-flash\",\n",
    "    \"openai/gpt-oss-20b\": \"GPT-OSS-20b\",\n",
    "    \"Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\": \"Qwen3-235B-A22B-Instruct-2507\",\n",
    "    \"gpt-5-nano-2025-08-07\": \"GPT-5-Nano\",\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\": \"Qwen3-30B-A3B-Instruct-2507\",\n",
    "    \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\": \"Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    \"gemini-2.5-flash-lite\": \"gemini-2.5-flash-lite\",\n",
    "    \"gpt-4.1-mini-2025-04-14\": \"GPT-4.1-mini\",\n",
    "    \"google/gemma-3-27b-it\": \"gemma-3-27b-it\",\n",
    "    \"mistralai/Mistral-Large-Instruct-2411\": \"Mistral-Large-Instruct-2411\",\n",
    "    \"google/gemma-3-12b-it\": \"gemma-3-12b-it\",\n",
    "    \"gpt-4.1-nano-2025-04-14\": \"GPT-4.1-Nano\",\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\": \"Qwen3-4B-Instruct-2507\",\n",
    "    \"RedHatAI/Llama-3.3-70B-Instruct-quantized.w8a8\": \"Llama-3.3-70B-Instruct\",\n",
    "    \"google/gemma-3-4b-it\": \"gemma-3-4b-it\",\n",
    "    \"mistralai/Ministral-8B-Instruct-2410\": \"Ministral-8B-Instruct-2410\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\": \"Llama-3.1-8B-Instruct\",\n",
    "    \"google/gemma-3-1b-it\": \"gemma-3-1b-it\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": \"Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": \"Llama-3.2-1B-Instruct\",\n",
    "    \"google/gemma-3-270m-it\": \"gemma-3-270m-it\",\n",
    "}\n",
    "\n",
    "model_order = [\n",
    "    \"GPT-5-Nano\",\n",
    "    \"GPT-5-mini\",\n",
    "    \"GPT-4.1-Nano\",\n",
    "    \"GPT-4.1-mini\",\n",
    "    \"GPT-OSS-120b\",\n",
    "    \"GPT-OSS-20b\",\n",
    "    \"Llama-3.3-70B-Instruct\",\n",
    "    \"Llama-3.1-8B-Instruct\",\n",
    "    \"Llama-3.2-3B-Instruct\",\n",
    "    \"Llama-3.2-1B-Instruct\",\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "    \"gemma-3-27b-it\",\n",
    "    \"gemma-3-12b-it\",\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-1b-it\",\n",
    "    \"gemma-3-270m-it\",\n",
    "    \"Mistral-Large-Instruct-2411\",\n",
    "    \"Mistral-Small-3.2-24B-Instruct-2506\",\n",
    "    \"Ministral-8B-Instruct-2410\",\n",
    "    \"Qwen3-235B-A22B-Instruct-2507\",\n",
    "    \"Qwen3-30B-A3B-Instruct-2507\",\n",
    "    \"Qwen3-4B-Instruct-2507\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcaf715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significance_level(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return \"***\"\n",
    "    elif p_value < 0.01:\n",
    "        return \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a376c3",
   "metadata": {},
   "source": [
    "# Lexical Perturbations\n",
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6777b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Identify changed rows\n",
    "mmlu_data = \"../../data/mmlu\"\n",
    "\n",
    "with open(os.path.join(mmlu_data, \"original.json\")) as f:\n",
    "    original_data = json.load(f)\n",
    "original_data_df = pd.DataFrame(original_data[\"data\"])\n",
    "\n",
    "with open(os.path.join(mmlu_data, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if row[\"question\"] != original_data_df.iloc[index][\"question\"] or row[\"choices\"] != original_data_df.iloc[index][\"choices\"]:\n",
    "        changed_rows.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7459b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Execute McNemar's test for paired nominal data for each model\n",
    "model_names = []\n",
    "p_values = []\n",
    "test_statistics = []\n",
    "results_dir = \"../../results\"\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"mmlu\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        original_results_df[\"correct_original\"] = original_results_df[\"prediction\"] == original_results_df[\"answer\"]\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"mmlu\", \"lexical\", \"llm_synonym.json\")) as f:\n",
    "            lexical_results = json.load(f)\n",
    "        lexical_results_df = pd.DataFrame(lexical_results[\"predictions\"]).iloc[changed_rows]\n",
    "        lexical_results_df[\"correct_lexical\"] = lexical_results_df[\"prediction\"] == lexical_results_df[\"answer\"]\n",
    "\n",
    "        # Contingency table\n",
    "        contingency_table = pd.crosstab(original_results_df[\"correct_original\"], lexical_results_df[\"correct_lexical\"])\n",
    "\n",
    "        # McNemar's test\n",
    "        test_result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "        p_values.append(test_result.pvalue)\n",
    "        test_statistics.append(test_result.statistic)\n",
    "\n",
    "mmlu_lexical_significance_df = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"p_value\": p_values,\n",
    "    \"test_statistic\": test_statistics\n",
    "})\n",
    "mmlu_lexical_significance_df[\"model\"] = mmlu_lexical_significance_df[\"model\"].map(id_to_model)\n",
    "mmlu_lexical_significance_df = mmlu_lexical_significance_df.set_index(\"model\").reindex(model_order).reset_index()\n",
    "mmlu_lexical_significance_df[\"significance_level\"] = mmlu_lexical_significance_df[\"p_value\"].apply(get_significance_level)\n",
    "mmlu_lexical_significance_df.to_csv(\"../../data/result_tables/mmlu_lexical_significance.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754025f",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7cfbf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Identify changed rows\n",
    "squad_data = \"../../data/squad\"\n",
    "original_data_df = squad_dataset = load_dataset(\"rajpurkar/squad\", split=\"validation\").shuffle(seed=77).select(range(1000)).to_pandas()\n",
    "with open(os.path.join(squad_data, \"lexical\", \"llm_synonym_perturbation.json\")) as f:\n",
    "    lexical_data = json.load(f)\n",
    "lexical_data_df = pd.DataFrame(lexical_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in lexical_data_df.iterrows():\n",
    "    if (row[\"question\"] != original_data_df.iloc[index][\"question\"]) or (row[\"context\"] != original_data_df.iloc[index][\"context\"]):\n",
    "        changed_rows.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa16d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dac8adc7a554ea28f97f1af401c59e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Initialize evaluators\n",
    "squad_evaluator = evaluate.load(\"squad\")\n",
    "\n",
    "# Execute McNemar's test for paired nominal data for each model on exact match metric\n",
    "model_names = []\n",
    "p_values = []\n",
    "test_statistics = []\n",
    "results_dir = \"../../results\"\n",
    "for model in tqdm(os.listdir(results_dir), desc=\"Processing models\"):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "\n",
    "        correct_original = []\n",
    "        for idx, row in original_results_df.iterrows():\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            em = squad_eval[\"exact_match\"]\n",
    "            if int(em) == 100:\n",
    "                correct_original.append(True)\n",
    "            else:\n",
    "                correct_original.append(False)\n",
    "        original_results_df[\"correct_original\"] = correct_original\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"lexical\", \"llm_synonym.json\")) as f:\n",
    "            lexical_results = json.load(f)\n",
    "        lexical_results_df = pd.DataFrame(lexical_results[\"predictions\"]).iloc[changed_rows]\n",
    "        correct_lexical = []\n",
    "        for idx, row in lexical_results_df.iterrows():\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            em = squad_eval[\"exact_match\"]\n",
    "            if int(em) == 100:\n",
    "                correct_lexical.append(True)\n",
    "            else:\n",
    "                correct_lexical.append(False)\n",
    "        lexical_results_df[\"correct_lexical\"] = correct_lexical\n",
    "\n",
    "        # Contingency table\n",
    "        contingency_table = pd.crosstab(original_results_df[\"correct_original\"], lexical_results_df[\"correct_lexical\"])\n",
    "\n",
    "        # McNemar's test\n",
    "        test_result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "        p_values.append(test_result.pvalue)\n",
    "        test_statistics.append(test_result.statistic)\n",
    "\n",
    "squad_lexical_significance_df = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"p_value\": p_values,\n",
    "    \"test_statistic\": test_statistics\n",
    "})\n",
    "squad_lexical_significance_df[\"model\"] = squad_lexical_significance_df[\"model\"].map(id_to_model)\n",
    "squad_lexical_significance_df = squad_lexical_significance_df.set_index(\"model\").reindex(model_order).reset_index()\n",
    "squad_lexical_significance_df[\"significance_level\"] = squad_lexical_significance_df[\"p_value\"].apply(get_significance_level)\n",
    "squad_lexical_significance_df.to_csv(\"../../data/result_tables/squad_lexical_significance.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893abf76",
   "metadata": {},
   "source": [
    "## AMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff88bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Execute McNemar's test for paired nominal data for each model on criterion satisfaction\n",
    "model_names = []\n",
    "p_values = []\n",
    "test_statistics = []\n",
    "results_dir = \"../../results\"\n",
    "criteria_file = \"../../AMEGA-benchmark/data/criteria.csv\"\n",
    "criteria_df = pd.read_csv(criteria_file, sep=\";\", decimal=\",\")\n",
    "criteria_df[\"criteria_score_possible\"] = criteria_df[\"criteria_score_possible\"].astype(float)\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"original_v3.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"])\n",
    "\n",
    "        correct_original = list(chain.from_iterable(original_results_df[\"majority_vote\"].values))\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"lexical\", \"llm_synonym_v3.json\")) as f:\n",
    "            lexical_results = json.load(f)\n",
    "        lexical_results_df = pd.DataFrame(lexical_results[\"predictions\"])\n",
    "\n",
    "        correct_lexical = list(chain.from_iterable(lexical_results_df[\"majority_vote\"].values))\n",
    "\n",
    "        # Contingency table\n",
    "        contingency_table = [[0, 0], [0, 0]]\n",
    "        for original, lexical, weight in zip(correct_original, correct_lexical, criteria_df[\"criteria_score_possible\"].values):\n",
    "            if original and lexical:\n",
    "                contingency_table[0][0] += weight\n",
    "            elif original and not lexical:\n",
    "                contingency_table[0][1] += weight\n",
    "            elif not original and lexical:\n",
    "                contingency_table[1][0] += weight\n",
    "            elif not original and not lexical:\n",
    "                contingency_table[1][1] += weight\n",
    "\n",
    "        # McNemar's test\n",
    "        test_result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "        p_values.append(test_result.pvalue)\n",
    "        test_statistics.append(test_result.statistic)\n",
    "\n",
    "amega_lexical_significance_df = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"p_value\": p_values,\n",
    "    \"test_statistic\": test_statistics\n",
    "})\n",
    "amega_lexical_significance_df[\"model\"] = amega_lexical_significance_df[\"model\"].map(id_to_model)\n",
    "amega_lexical_significance_df = amega_lexical_significance_df.set_index(\"model\").reindex(model_order).reset_index()\n",
    "amega_lexical_significance_df[\"significance_level\"] = amega_lexical_significance_df[\"p_value\"].apply(get_significance_level)\n",
    "amega_lexical_significance_df.to_csv(\"../../data/result_tables/amega_lexical_significance.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e479e",
   "metadata": {},
   "source": [
    "# Syntactic Perturbations\n",
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87fada87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Identify changed rows\n",
    "mmlu_data = \"../../data/mmlu\"\n",
    "\n",
    "with open(os.path.join(mmlu_data, \"original.json\")) as f:\n",
    "    original_data = json.load(f)\n",
    "original_data_df = pd.DataFrame(original_data[\"data\"])\n",
    "\n",
    "with open(os.path.join(mmlu_data, \"syntactic\", \"syntactic_perturbation.json\")) as f:\n",
    "    syntactic_data = json.load(f)\n",
    "syntactic_data_df = pd.DataFrame(syntactic_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in syntactic_data_df.iterrows():\n",
    "    if row[\"question\"] != original_data_df.iloc[index][\"question\"] or row[\"choices\"] != original_data_df.iloc[index][\"choices\"]:\n",
    "        changed_rows.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b07e93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute McNemar's test for paired nominal data for each model\n",
    "model_names = []\n",
    "p_values = []\n",
    "test_statistics = []\n",
    "results_dir = \"../../results\"\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"mmlu\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "        original_results_df[\"correct_original\"] = original_results_df[\"prediction\"] == original_results_df[\"answer\"]\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"mmlu\", \"syntactic\", \"syntactic.json\")) as f:\n",
    "            syntactic_results = json.load(f)\n",
    "        syntactic_results_df = pd.DataFrame(syntactic_results[\"predictions\"]).iloc[changed_rows]\n",
    "        syntactic_results_df[\"correct_syntactic\"] = syntactic_results_df[\"prediction\"] == syntactic_results_df[\"answer\"]\n",
    "\n",
    "        # Contingency table\n",
    "        contingency_table = pd.crosstab(original_results_df[\"correct_original\"], syntactic_results_df[\"correct_syntactic\"])\n",
    "\n",
    "        # McNemar's test\n",
    "        test_result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "        p_values.append(test_result.pvalue)\n",
    "        test_statistics.append(test_result.statistic)\n",
    "\n",
    "mmlu_syntactic_significance_df = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"p_value\": p_values,\n",
    "    \"test_statistic\": test_statistics\n",
    "})\n",
    "mmlu_syntactic_significance_df[\"model\"] = mmlu_syntactic_significance_df[\"model\"].map(id_to_model)\n",
    "mmlu_syntactic_significance_df = mmlu_syntactic_significance_df.set_index(\"model\").reindex(model_order).reset_index()\n",
    "mmlu_syntactic_significance_df[\"significance_level\"] = mmlu_syntactic_significance_df[\"p_value\"].apply(get_significance_level)\n",
    "mmlu_syntactic_significance_df.to_csv(\"../../data/result_tables/mmlu_syntactic_significance.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61b87d",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "331f1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify changed rows\n",
    "squad_data = \"../../data/squad\"\n",
    "original_data_df = squad_dataset = load_dataset(\"rajpurkar/squad\", split=\"validation\").shuffle(seed=77).select(range(1000)).to_pandas()\n",
    "with open(os.path.join(squad_data, \"syntactic\", \"syntactic_perturbation.json\")) as f:\n",
    "    syntactic_data = json.load(f)\n",
    "syntactic_data_df = pd.DataFrame(syntactic_data[\"data\"])\n",
    "\n",
    "changed_rows = []\n",
    "for index, row in syntactic_data_df.iterrows():\n",
    "    if (row[\"question\"] != original_data_df.iloc[index][\"question\"]) or (row[\"context\"] != original_data_df.iloc[index][\"context\"]):\n",
    "        changed_rows.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7673150a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e5cc3539624dd89c9c62da8f1d0532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing models:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute McNemar's test for paired nominal data for each model on exact match metric\n",
    "model_names = []\n",
    "p_values = []\n",
    "test_statistics = []\n",
    "results_dir = \"../../results\"\n",
    "for model in tqdm(os.listdir(results_dir), desc=\"Processing models\"):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"original.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"]).iloc[changed_rows]\n",
    "\n",
    "        correct_original = []\n",
    "        for idx, row in original_results_df.iterrows():\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            em = squad_eval[\"exact_match\"]\n",
    "            if int(em) == 100:\n",
    "                correct_original.append(True)\n",
    "            else:\n",
    "                correct_original.append(False)\n",
    "        original_results_df[\"correct_original\"] = correct_original\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"squad\", \"syntactic\", \"syntactic.json\")) as f:\n",
    "            syntactic_results = json.load(f)\n",
    "        syntactic_results_df = pd.DataFrame(syntactic_results[\"predictions\"]).iloc[changed_rows]\n",
    "        correct_syntactic = []\n",
    "        for idx, row in syntactic_results_df.iterrows():\n",
    "            squad_eval = squad_evaluator.compute(\n",
    "                predictions=[{\"id\": str(idx), \"prediction_text\": row[\"prediction\"]}],\n",
    "                references=[{\"id\": str(idx), \"answers\": {\"text\": row[\"answers\"], \"answer_start\": [0] * len(row[\"answers\"])}}]\n",
    "            )\n",
    "            em = squad_eval[\"exact_match\"]\n",
    "            if int(em) == 100:\n",
    "                correct_syntactic.append(True)\n",
    "            else:\n",
    "                correct_syntactic.append(False)\n",
    "        syntactic_results_df[\"correct_syntactic\"] = correct_syntactic\n",
    "\n",
    "        # Contingency table\n",
    "        contingency_table = pd.crosstab(original_results_df[\"correct_original\"], syntactic_results_df[\"correct_syntactic\"])\n",
    "\n",
    "        # McNemar's test\n",
    "        test_result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "        p_values.append(test_result.pvalue)\n",
    "        test_statistics.append(test_result.statistic)\n",
    "\n",
    "squad_syntactic_significance_df = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"p_value\": p_values,\n",
    "    \"test_statistic\": test_statistics\n",
    "})\n",
    "squad_syntactic_significance_df[\"model\"] = squad_syntactic_significance_df[\"model\"].map(id_to_model)\n",
    "squad_syntactic_significance_df = squad_syntactic_significance_df.set_index(\"model\").reindex(model_order).reset_index()\n",
    "squad_syntactic_significance_df[\"significance_level\"] = squad_syntactic_significance_df[\"p_value\"].apply(get_significance_level)\n",
    "squad_syntactic_significance_df.to_csv(\"../../data/result_tables/squad_syntactic_significance.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec6069",
   "metadata": {},
   "source": [
    "## AMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc6a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute McNemar's test for paired nominal data for each model on criterion satisfaction\n",
    "model_names = []\n",
    "p_values = []\n",
    "test_statistics = []\n",
    "results_dir = \"../../results\"\n",
    "criteria_file = \"../../AMEGA-benchmark/data/criteria.csv\"\n",
    "criteria_df = pd.read_csv(criteria_file, sep=\";\", decimal=\",\")\n",
    "criteria_df[\"criteria_score_possible\"] = criteria_df[\"criteria_score_possible\"].astype(float)\n",
    "for model in os.listdir(results_dir):\n",
    "    if os.path.isdir(os.path.join(results_dir, model)):\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"original_v3.json\")) as f:\n",
    "            original_results = json.load(f)\n",
    "        model_names.append(original_results[\"model\"])\n",
    "        original_results_df = pd.DataFrame(original_results[\"predictions\"])\n",
    "        correct_original = list(chain.from_iterable(original_results_df[\"majority_vote\"].values))\n",
    "\n",
    "        with open(os.path.join(results_dir, model, \"amega\", \"syntactic\", \"syntactic_v3.json\")) as f:\n",
    "            syntactic_results = json.load(f)\n",
    "        syntactic_results_df = pd.DataFrame(syntactic_results[\"predictions\"])\n",
    "        correct_syntactic = list(chain.from_iterable(syntactic_results_df[\"majority_vote\"].values))\n",
    "\n",
    "        # Contingency table\n",
    "        contingency_table = [[0, 0], [0, 0]]\n",
    "        for original, syntactic, weight in zip(correct_original, correct_syntactic, criteria_df[\"criteria_score_possible\"].values):\n",
    "            if original and syntactic:\n",
    "                contingency_table[0][0] += weight\n",
    "            elif original and not syntactic:\n",
    "                contingency_table[0][1] += weight\n",
    "            elif not original and syntactic:\n",
    "                contingency_table[1][0] += weight\n",
    "            elif not original and not syntactic:\n",
    "                contingency_table[1][1] += weight\n",
    "\n",
    "        # McNemar's test\n",
    "        test_result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "        p_values.append(test_result.pvalue)\n",
    "        test_statistics.append(test_result.statistic)\n",
    "\n",
    "amega_syntactic_significance_df = pd.DataFrame({\n",
    "    \"model\": model_names,\n",
    "    \"p_value\": p_values,\n",
    "    \"test_statistic\": test_statistics\n",
    "})\n",
    "amega_syntactic_significance_df[\"model\"] = amega_syntactic_significance_df[\"model\"].map(id_to_model)\n",
    "amega_syntactic_significance_df = amega_syntactic_significance_df.set_index(\"model\").reindex(model_order).reset_index()\n",
    "amega_syntactic_significance_df[\"significance_level\"] = amega_syntactic_significance_df[\"p_value\"].apply(get_significance_level)\n",
    "amega_syntactic_significance_df.to_csv(\"../../data/result_tables/amega_syntactic_significance.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
